@article{912453c6-2f62-38a3-a437-daafdf84c5e6,
 ISSN = {01621459},
 URL = {http://www.jstor.org/stable/2286841},
 abstract = {Aspects of scientific method are discussed: In particular, its representation as a motivated iteration in which, in succession, practice confronts theory, and theory, practice. Rapid progress requires sufficient flexibility to profit from such confrontations, and the ability to devise parsimonious but effective models, to worry selectively about model inadequacies and to employ mathematics skillfully but appropriately. The development of statistical methods at Rothamsted Experimental Station by Sir Ronald Fisher is used to illustrate these themes.},
 author = {George E. P. Box},
 journal = {Journal of the American Statistical Association},
 number = {356},
 pages = {791--799},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd.]},
 title = {Science and Statistics},
 urldate = {2023-09-07},
 volume = {71},
 year = {1976}
}

@inproceedings{
caballero2023broken,
title={Broken Neural Scaling Laws},
author={Ethan Caballero and Kshitij Gupta and Irina Rish and David Krueger},
booktitle={The Eleventh International Conference on Learning Representations },
year={2023},
url={https://openreview.net/forum?id=sckjveqlCZ}
}

@article{buchberger1965algorithmus,
  title={Ein Algorithmus zum Auffinden der Basiselemente des Restklassenringes nach einem nulldimensionalen Polynomideal},
  author={Buchberger, Bruno},
  journal={Ph. D. Thesis, Math. Inst., University of Innsbruck},
  year={1965}
}

@misc{hoffmann2022training,
      title={Training Compute-Optimal Large Language Models}, 
      author={Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
      year={2022},
      eprint={2203.15556},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@Book{pearl:88,
  author = 	 {Judea Pearl},
  title = 	 {Probabilistic {R}easoning in {I}ntelligent {S}ystems: 
		  {N}etworks of {P}lausible {I}nference},
  publisher = 	 {Morgan Kaufman Publishers},
  year = 	 {1988},
  address = 	 {San Mateo, CA}
}

@inproceedings{nakkiran2020optimal,
  title     = {Optimal Regularization can Mitigate Double Descent},
  author    = {Nakkiran, Preetum and Venkat, Prayaag and Kakade, Sham M and Ma, Tengyu},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@book{potters2020first,
  title     = {A First Course in Random Matrix Theory: For Physicists, Engineers and Data Scientists},
  author    = {Potters, Marc and Bouchaud, Jean-Philippe},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@inproceedings{woodworth2020kernel,
  title        = {Kernel and rich regimes in overparametrized models},
  author       = {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
  booktitle    = {Conference on Learning Theory},
  pages        = {3635--3673},
  year         = {2020},
  organization = {PMLR}
}

@article{marcus2022finite,
  title={Finite free convolutions of polynomials},
  author={Marcus, Adam W and Spielman, Daniel A and Srivastava, Nikhil},
  journal={Probability Theory and Related Fields},
  volume={182},
  number={3-4},
  pages={807--848},
  year={2022},
  publisher={Springer}
}
@article{ising1925beitrag,
  title={Beitrag zur theorie des ferromagnetismus},
  author={Ising, Ernst},
  journal={Zeitschrift f{\"u}r Physik},
  volume={31},
  number={1},
  pages={253--258},
  year={1925},
  publisher={Springer}
}

@article{meir1994bias,
  title   = {Bias, variance and the combination of least squares estimators},
  author  = {Meir, Ronny},
  journal = {Advances in neural information processing systems},
  volume  = {7},
  year    = {1994}
}

@article{belkin2020two,
  title     = {Two models of double descent for weak features},
  author    = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  journal   = {SIAM Journal on Mathematics of Data Science},
  volume    = {2},
  number    = {4},
  pages     = {1167--1180},
  year      = {2020},
  publisher = {SIAM}
}

@article{PhysRevE.98.062120,
  title     = {Path integral approach to random neural networks},
  author    = {Crisanti, A. and Sompolinsky, H.},
  journal   = {Phys. Rev. E},
  volume    = {98},
  issue     = {6},
  pages     = {062120},
  numpages  = {16},
  year      = {2018},
  month     = {Dec},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevE.98.062120},
  url       = {https://link.aps.org/doi/10.1103/PhysRevE.98.062120}
}


@article{PhysRevLett.61.259,
  title     = {Chaos in Random Neural Networks},
  author    = {Sompolinsky, H. and Crisanti, A. and Sommers, H. J.},
  journal   = {Phys. Rev. Lett.},
  volume    = {61},
  issue     = {3},
  pages     = {259--262},
  numpages  = {0},
  year      = {1988},
  month     = {Jul},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.61.259},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.61.259}
}


@book{parisi2020theory,
  title     = {Theory of Simple Glasses: Exact Solutions in Infinite Dimensions},
  author    = {Parisi, G. and Urbani, P. and Zamponi, F.},
  isbn      = {9781107191075},
  lccn      = {2019038319},
  url       = {https://books.google.ch/books?id=qkCUxgEACAAJ},
  year      = {2020},
  publisher = {Cambridge University Press}
}

@article{Biroli2018,
  title     = {Out-of-equilibrium dynamical mean-field equations for the perceptron model},
  volume    = {51},
  issn      = {1751-8121},
  url       = {http://dx.doi.org/10.1088/1751-8121/aaa68d},
  doi       = {10.1088/1751-8121/aaa68d},
  number    = {8},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  publisher = {IOP Publishing},
  author    = {Agoritsas, Elisabeth and Biroli, Giulio and Urbani, Pierfrancesco and Zamponi, Francesco},
  year      = {2018},
  month     = {Jan},
  pages     = {085002}
}



@article{Mignacco-et-al2021,
  title     = {Stochasticity helps to navigate rough landscapes: comparing gradient-descent-based algorithms in the phase retrieval problem},
  author    = {Mignacco, Francesca and Urbani, Pierfrancesco and Zdeborov{\'a}, Lenka},
  journal   = {Machine Learning: Science and Technology},
  year      = {2021},
  publisher = {IOP Publishing}
}

@inproceedings{NEURIPS2020_6c81c83c,
  author    = {Mignacco, Francesca and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov\'{a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {9540--9550},
  publisher = {Curran Associates, Inc.},
  title     = {Dynamical mean-field theory for stochastic gradient descent in Gaussian mixture classification},
  url       = {https://proceedings.neurips.cc/paper/2020/file/6c81c83c4bd0b58850495f603ab45a93-Paper.pdf},
  volume    = {33},
  year      = {2020}
}


@article{Sarao_Mannelli_2020,
  title     = {Marvels and Pitfalls of the {L}angevin Algorithm in Noisy High-Dimensional Inference},
  volume    = {10},
  issn      = {2160-3308},
  url       = {http://dx.doi.org/10.1103/PhysRevX.10.011057},
  doi       = {10.1103/physrevx.10.011057},
  number    = {1},
  journal   = {Physical Review X},
  publisher = {American Physical Society (APS)},
  author    = {Sarao Mannelli, Stefano and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborov{\`a}, Lenka},
  year      = {2020},
  month     = {Mar}
}

@article{de2022random,
  title={A random matrix perspective on random tensors},
  author={de Morais Goulart, Jos{\'e} Henrique and Couillet, Romain and Comon, Pierre},
  journal={The Journal of Machine Learning Research},
  volume={23},
  number={1},
  pages={12110--12145},
  year={2022},
  publisher={JMLRORG}
}


@inproceedings{mannelli2019passed,
  title     = {Passed and Spurious: Descent Algorithms and Local Minima in Spiked Matrix-Tensor Models},
  author    = {Mannelli, Stefano Sarao and Krzakala, Florent and Urbani, Pierfrancesco and Zdeborova, Lenka},
  pages     = {4333--4342},
  year      = {2019},
  editor    = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
  volume    = {97},
  series    = {Proceedings of Machine Learning Research},
  address   = {Long Beach, California, USA},
  month     = {09--15 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v97/mannelli19a/mannelli19a.pdf},
  url       = {http://proceedings.mlr.press/v97/mannelli19a.html},
  abstract  = {In this work we analyse quantitatively the interplay between the loss landscape and performance of descent algorithms in a prototypical inference problem, the spiked matrix-tensor model. We study a loss function that is the negative log-likelihood of the model. We analyse the number of local minima at a fixed distance from the signal/spike with the Kac-Rice formula, and locate trivialization of the landscape at large signal-to-noise ratios. We evaluate analytically the performance of a gradient flow algorithm using integro-differential PDEs as developed in physics of disordered systems for the Langevin dynamics. We analyze the performance of an approximate message passing algorithm estimating the maximum likelihood configuration via its state evolution. We conclude by comparing the above results: while we observe a drastic slow down of the gradient flow dynamics even in the region where the landscape is trivial, both the analyzed algorithms are shown to perform well even in the part of the region of parameters where spurious local minima are present.}
}


@incollection{mannelli2019afraid,
  title     = {Who is Afraid of Big Bad Minima? Analysis of gradient-flow in spiked matrix-tensor models},
  author    = {Sarao Mannelli, Stefano and Biroli, Giulio and Cammarota, Chiara and Krzakala, Florent and Zdeborov\'{a}, Lenka},
  booktitle = {Advances in Neural Information Processing Systems 32},
  editor    = {H. Wallach and H. Larochelle and A. Beygelzimer and F. Alch\'{e}-Buc and E. Fox and R. Garnett},
  pages     = {8679--8689},
  year      = {2019},
  publisher = {Curran Associates, Inc.}
}



@article{BADAGA,
  author  = {Ben Arous, G{\'e}rard and Dembo, Amir and Guionnet, Alice},
  year    = {2004},
  month   = {10},
  pages   = {},
  title   = {Cugliandolo-Kurchan equations for dynamics of Spin-Glasses},
  volume  = {136},
  journal = {Probability Theory and Related Fields},
  doi     = {10.1007/s00440-005-0491-y}
}



@article{PhysRevLett.71.173,
  title     = {Analytical solution of the off-equilibrium dynamics of a long-range spin-glass model},
  author    = {Cugliandolo, L. F. and Kurchan, J.},
  journal   = {Phys. Rev. Lett.},
  volume    = {71},
  issue     = {1},
  pages     = {173--176},
  numpages  = {0},
  year      = {1993},
  month     = {Jul},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.71.173},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.71.173}
}



@article{Crisanti1993TheSI,
  title   = {The sphericalp-spin interaction spin-glass model},
  author  = {A. Crisanti and H. Horner and H. Sommers},
  journal = {Zeitschrift f{\"u}r Physik B Condensed Matter},
  year    = {1993},
  volume  = {92},
  pages   = {257-271}
}
@article{bodin2021model,
  title   = {Model, sample, and epoch-wise descents: exact solution of gradient flow in the random feature model},
  author  = {Bodin, Antoine and Macris, Nicolas},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  year    = {2021}
}

@inproceedings{brown2020language,
  author    = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {1877--1901},
  publisher = {Curran Associates, Inc.},
  title     = {Language Models are Few-Shot Learners},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
  volume    = {33},
  year      = {2020}
}


@article{kaplan2020scaling,
  author   = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
  title    = {Scaling Laws for Neural Language Models},
  journal  = {arXiv},
  volume   = {},
  number   = {},
  pages    = {2001.08361v1},
  year     = {2020},
  abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
  location = {},
  keywords = {}
}

 @inproceedings{adlam2020neural,
  title     = {The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization},
  author    = {Adlam, Ben and Pennington, Jeffrey},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {74--84},
  year      = {2020},
  editor    = {Hal Daume III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/adlam20a/adlam20a.pdf},
  url       = { http://proceedings.mlr.press/v119/adlam20a.html },
  abstract  = {Modern deep learning models employ considerably more parameters than required to fit the training data. Whereas conventional statistical wisdom suggests such models should drastically overfit, in practice these models generalize remarkably well. An emerging paradigm for describing this unexpected behavior is in terms of a \emph{double descent} curve, in which increasing a model�s capacity causes its test error to first decrease, then increase to a maximum near the interpolation threshold, and then decrease again in the overparameterized regime. Recent efforts to explain this phenomenon theoretically have focused on simple settings, such as linear regression or kernel regression with unstructured random features, which we argue are too coarse to reveal important nuances of actual neural networks. We provide a precise high-dimensional asymptotic analysis of generalization under kernel regression with the Neural Tangent Kernel, which characterizes the behavior of wide neural networks optimized with gradient descent. Our results reveal that the test error has nonmonotonic behavior deep in the overparameterized regime and can even exhibit additional peaks and descents when the number of parameters scales quadratically with the dataset size.}
} 

@article{edwards1976eigenvalue,
  title     = {The eigenvalue spectrum of a large symmetric random matrix},
  author    = {Edwards, Samuel F and Jones, Raymund C},
  journal   = {Journal of Physics A: Mathematical and General},
  volume    = {9},
  number    = {10},
  pages     = {1595},
  year      = {1976},
  publisher = {IOP Publishing}
}

@book{murphy2012machine,
  title={Machine learning: a probabilistic perspective},
  author={Murphy, Kevin P},
  year={2012},
  publisher={MIT press}
}

@article{tao2008random,
  title={Random matrices: the circular law},
  author={Tao, Terence and Vu, Van},
  journal={Communications in Contemporary Mathematics},
  volume={10},
  number={02},
  pages={261--307},
  year={2008},
  publisher={World Scientific}
}

@book{hastie01statisticallearning,
  added-at  = {2008-05-16T16:17:42.000+0200},
  address   = {New York, NY, USA},
  author    = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
  biburl    = {https://www.bibsonomy.org/bibtex/2f58afc5c9793fcc8ad8389824e57984c/sb3000},
  interhash = {d585aea274f2b9b228fc1629bc273644},
  intrahash = {f58afc5c9793fcc8ad8389824e57984c},
  keywords  = {ml statistics},
  publisher = {Springer New York Inc.},
  series    = {Springer Series in Statistics},
  timestamp = {2008-05-16T16:17:43.000+0200},
  title     = {The Elements of Statistical Learning},
  year      = 2001
}

@article{bai1997circular,
  title={Circular law},
  author={Bai, Zhi Dong},
  journal={The Annals of Probability},
  volume={25},
  number={1},
  pages={494--529},
  year={1997},
  publisher={Institute of Mathematical Statistics}
}

@book{james2013introduction,
  title={An introduction to statistical learning},
  author={James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert and others},
  volume={112},
  year={2013},
  publisher={Springer}
}



@article{DBLP:journals/corr/ZhangBHRV16,
  author        = {Chiyuan Zhang and
                   Samy Bengio and
                   Moritz Hardt and
                   Benjamin Recht and
                   Oriol Vinyals},
  title         = {Understanding deep learning requires rethinking generalization},
  journal       = {ICLR 2017},
  volume        = {arXiv abs/1611.03530},
  year          = {2016},
  url           = {http://arxiv.org/abs/1611.03530},
  archiveprefix = {arXiv},
  eprint        = {1611.03530},
  timestamp     = {Mon, 13 Aug 2018 16:47:02 +0200},
  biburl        = {https://dblp.org/rec/journals/corr/ZhangBHRV16.bib},
  bibsource     = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2021multiple,
  title   = {Multiple descent: Design your own generalization curve},
  author  = {Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {34},
  year    = {2021}
}

@article{BelkinPNAS2019,
  author  = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year    = {2019},
  month   = {07},
  pages   = {201903070},
  title   = {Reconciling modern machine-learning practice and the classical bias-variance trade-off},
  volume  = {116},
  journal = {Proceedings of the National Academy of Sciences},
  doi     = {10.1073/pnas.1903070116}
}

@inproceedings{pmlr-v80-belkin18a,
  title     = {To Understand Deep Learning We Need to Understand Kernel Learning},
  author    = {Belkin, Mikhail and Ma, Siyuan and Mandal, Soumik},
  booktitle = {Proceedings of the 35th International Conference on Machine Learning},
  pages     = {541--549},
  year      = {2018},
  editor    = {Dy, Jennifer and Krause, Andreas},
  volume    = {80},
  series    = {Proceedings of Machine Learning Research},
  month     = {10--15 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v80/belkin18a/belkin18a.pdf},
  url       = {http://proceedings.mlr.press/v80/belkin18a.html},
  abstract  = {Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, which are typically heavily over-parametrized, tend to fit the training data exactly. Despite this "overfitting", they perform well on test data, a phenomenon not yet fully understood. The first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification error or near zero regression error (interpolation) perform very well on test data. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. None of the existing bounds produce non-trivial results for interpolating solutions. We also show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results recently reported for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. Some key phenomena of deep learning are manifested similarly in kernel methods in the modern "overfitted" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable "shallow" kernel methods are better understood.}
}


@inproceedings{pmlr-v89-belkin19a,
  title     = {Does data interpolation contradict statistical optimality?},
  author    = {Belkin, Mikhail and Rakhlin, Alexander and Tsybakov, Alexandre B.},
  booktitle = {Proceedings of the Twenty-Second International Conference on Artificial Intelligence and Statistics},
  pages     = {1611--1619},
  year      = {2019},
  editor    = {Chaudhuri, Kamalika and Sugiyama, Masashi},
  volume    = {89},
  series    = {Proceedings of Machine Learning Research},
  month     = {16--18 Apr},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v89/belkin19a/belkin19a.pdf},
  url       = {http://proceedings.mlr.press/v89/belkin19a.html},
  abstract  = {We show that classical learning methods interpolating the training data can achieve optimal rates for the problems of nonparametric regression and prediction with square loss.}
}


@article{Belkin_2020,
  title     = {Two Models of Double Descent for Weak Features},
  volume    = {2},
  issn      = {2577-0187},
  url       = {http://dx.doi.org/10.1137/20M1336072},
  doi       = {10.1137/20m1336072},
  number    = {4},
  journal   = {SIAM Journal on Mathematics of Data Science},
  publisher = {Society for Industrial & Applied Mathematics (SIAM)},
  author    = {Belkin, Mikhail and Hsu, Daniel and Xu, Ji},
  year      = {2020},
  month     = {Jan},
  pages     = {1167-1180}
}

@article{bouchaud2002statistical,
  title     = {Statistical properties of stock order books: empirical results and models},
  author    = {Bouchaud, Jean-Philippe and M{\'e}zard, Marc and Potters, Marc},
  journal   = {Quantitative finance},
  volume    = {2},
  number    = {4},
  pages     = {251},
  year      = {2002},
  publisher = {IOP Publishing}
}

@article{Hastie-Montanari-2019,
  author        = {{Hastie}, Trevor and {Montanari}, Andrea and {Rosset}, Saharon and {Tibshirani}, Ryan J.},
  title         = {{Surprises in High-Dimensional Ridgeless Least Squares Interpolation}},
  journal       = {arXiv e-prints},
  keywords      = {Mathematics - Statistics Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
  year          = 2019,
  month         = mar,
  eid           = {arXiv:1903.08560},
  pages         = {arXiv:1903.08560},
  archiveprefix = {arXiv},
  eprint        = {1903.08560},
  primaryclass  = {math.ST},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190308560H},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{bun2017cleaning,
  title     = {Cleaning large correlation matrices: tools from random matrix theory},
  author    = {Bun, Jo{\"e}l and Bouchaud, Jean-Philippe and Potters, Marc},
  journal   = {Physics Reports},
  volume    = {666},
  pages     = {1--109},
  year      = {2017},
  publisher = {Elsevier}
}

@inproceedings{bordelon2020spectrum,
  title        = {Spectrum dependent learning curves in kernel regression and wide neural networks},
  author       = {Bordelon, Blake and Canatar, Abdulkadir and Pehlevan, Cengiz},
  booktitle    = {International Conference on Machine Learning},
  pages        = {1024--1034},
  year         = {2020},
  organization = {PMLR}
}

@article{ADVANI2020428,
  title    = {High-dimensional dynamics of generalization error in neural networks},
  journal  = {Neural Networks},
  volume   = {132},
  pages    = {428-446},
  year     = {2020},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.022},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608020303117},
  author   = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
  keywords = {Neural networks, Generalization error, Random matrix theory},
  abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.}
}

@book{erdHos2017dynamical,
  title     = {A dynamical approach to random matrix theory},
  author    = {Erd{\H{o}}s, L{\'a}szl{\'o} and Yau, Horng-Tzer},
  volume    = {28},
  year      = {2017},
  publisher = {American Mathematical Soc.}
}

@article{benaych2016lectures,
  title   = {Lectures on the local semicircle law for Wigner matrices},
  author  = {Benaych-Georges, Florent and Knowles, Antti},
  journal = {arXiv preprint arXiv:1601.04055},
  year    = {2016}
}

@article{Sahai-et-al,
  author  = {Muthukumar, Vidya and Vodrahalli, Kailas and Subramanian, Vignesh and Sahai, Anant},
  journal = {IEEE Journal on Selected Areas in Information Theory},
  title   = {Harmless Interpolation of Noisy Data in Regression},
  year    = {2020},
  volume  = {1},
  number  = {1},
  pages   = {67-83},
  doi     = {10.1109/JSAIT.2020.2984716}
}
  
  @article{Thrampoulidis2021,
  author   = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  title    = {{A model of double descent for high-dimensional binary linear classification}},
  journal  = {Information and Inference: A Journal of the IMA},
  year     = {2021},
  month    = {04},
  abstract = {{We consider a model for logistic regression where only a subset of features of size \\$p\\$ is used for training a linear classifier over \\$n\\$ training samples. The classifier is obtained by running gradient descent on logistic loss. For this model, we investigate the dependence of the classification error on the ratio \\$\\kappa =p/n\\$. First, building on known deterministic results on the implicit bias of gradient descent, we uncover a phase-transition phenomenon for the case of Gaussian features: the classification error of the gradient descent solution is the same as that of the maximum-likelihood solution when \\$\\kappa \\&lt;\\kappa \_\\star \\$, and that of the support vector machine when \\$\\kappa\\&gt;\\kappa \_\\star \\$, where \\$\\kappa \_\\star \\$ is a phase-transition threshold. Next, using the convex Gaussian min–max theorem, we sharply characterize the performance of both the maximum-likelihood and the support vector machine solutions. Combining these results, we obtain curves that explicitly characterize the classification error for varying values of \\$\\kappa \\$. The numerical results validate the theoretical predictions and unveil double-descent phenomena that complement similar recent findings in linear regression settings as well as empirical observations in more complex learning scenarios.}},
  issn     = {2049-8772},
  doi      = {10.1093/imaiai/iaab002},
  url      = {https://doi.org/10.1093/imaiai/iaab002},
  note     = {iaab002},
  eprint   = {https://academic.oup.com/imaiai/advance-article-pdf/doi/10.1093/imaiai/iaab002/36872804/iaab002.pdf}
}


@inproceedings{NEURIPS2020_37740d59,
  author    = {Derezinski, Michal and Liang, Feynman T and Mahoney, Michael W},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {5152--5164},
  publisher = {Curran Associates, Inc.},
  title     = {Exact expressions for double descent and implicit regularization via surrogate random design},
  url       = {https://proceedings.neurips.cc/paper/2020/file/37740d59bb0eb7b4493725b2e0e5289b-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@article{Spigler_2019,
  title     = {A jamming transition from under- to over-parametrization affects generalization in deep learning},
  volume    = {52},
  issn      = {1751-8121},
  url       = {http://dx.doi.org/10.1088/1751-8121/ab4c8b},
  doi       = {10.1088/1751-8121/ab4c8b},
  number    = {47},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  publisher = {IOP Publishing},
  author    = {Spigler, S and Geiger, M and d'Ascoli, S and Sagun, L and Biroli, G and Wyart, M},
  year      = {2019},
  month     = {Oct},
  pages     = {474001}
}

  
@article{Bartlett30063,
  author    = {Bartlett, Peter L. and Long, Philip M. and Lugosi, G{\'a}bor and Tsigler, Alexander},
  title     = {Benign overfitting in linear regression},
  volume    = {117},
  number    = {48},
  pages     = {30063--30070},
  year      = {2020},
  doi       = {10.1073/pnas.1907378117},
  publisher = {National Academy of Sciences},
  abstract  = {The phenomenon of benign overfitting is one of the key mysteries uncovered by deep learning methodology: deep neural networks seem to predict well, even with a perfect fit to noisy training data. Motivated by this phenomenon, we consider when a perfect fit to training data in linear regression is compatible with accurate prediction. We give a characterization of linear regression problems for which the minimum norm interpolating prediction rule has near-optimal prediction accuracy. The characterization is in terms of two notions of the effective rank of the data covariance. It shows that overparameterization is essential for benign overfitting in this setting: the number of directions in parameter space that are unimportant for prediction must significantly exceed the sample size. By studying examples of data covariance properties that this characterization shows are required for benign overfitting, we find an important role for finite-dimensional data: the accuracy of the minimum norm interpolating prediction rule approaches the best possible accuracy for a much narrower range of properties of the data distribution when the data lie in an infinite-dimensional space vs. when the data lie in a finite-dimensional space with dimension that grows faster than the sample size.},
  issn      = {0027-8424},
  url       = {https://www.pnas.org/content/117/48/30063},
  eprint    = {https://www.pnas.org/content/117/48/30063.full.pdf},
  journal   = {Proceedings of the National Academy of Sciences}
}

@inproceedings{liao:hal-02971807,
  title       = {{A random matrix analysis of random Fourier features: beyond the Gaussian kernel, a precise phase transition, and the corresponding double descent}},
  author      = {Liao, Zhenyu and Couillet, Romain and Mahoney, Michael W},
  url         = {https://hal.archives-ouvertes.fr/hal-02971807},
  booktitle   = {{34th Conference on Neural Information Processing Systems (NeurIPS 2020)}},
  address     = {Vancouver, Canada},
  year        = {2020},
  month       = Dec,
  pdf         = {https://hal.archives-ouvertes.fr/hal-02971807/file/main.pdf},
  hal_id      = {hal-02971807},
  hal_version = {v1}
}

@inproceedings{pmlr-v119-gerace20a,
  title     = {Generalisation error in learning with random features and the hidden manifold model},
  author    = {Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and Mezard, Marc and Zdeborova, Lenka},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {3452--3462},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/gerace20a/gerace20a.pdf},
  url       = {
               http://proceedings.mlr.press/v119/gerace20a.html
               },
  abstract  = {We study generalised linear regression and classification for a synthetically generated dataset encompassing different problems of interest, such as learning with random features, neural networks in the lazy training regime, and the hidden manifold model. We consider the high-dimensional regime and using the replica method from statistical physics, we provide a closed-form expression for the asymptotic generalisation performance in these problems, valid in both the under- and over-parametrised regimes and for a broad choice of generalised linear model loss functions. In particular, we show how to obtain analytically the so-called double descent behaviour for logistic regression with a peak at the interpolation threshold, we illustrate the superiority of orthogonal against random Gaussian projections in learning with random features, and discuss the role played by correlations in the data generated by the hidden manifold model. Beyond the interest in these particular problems, the theoretical formalism introduced in this manuscript provides a path to further extensions to more complex tasks.}
}

@article{dhifallah2020precise,
  title={A precise performance analysis of learning with random features},
  author={Dhifallah, Oussama and Lu, Yue M},
  journal={arXiv preprint arXiv:2008.11904},
  year={2020}
}

@inproceedings{pmlr-v119-d-ascoli20a,
  title     = {Double Trouble in Double Descent: Bias and Variance(s) in the Lazy Regime},
  author    = {D'Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {2280--2290},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/d-ascoli20a/d-ascoli20a.pdf},
  url       = { http://proceedings.mlr.press/v119/d-ascoli20a.html }
} 
 
@article{mei2020generalization,
  author        = {{Mei}, Song and {Montanari}, Andrea},
  title         = {{The generalization error of random features regression: Precise asymptotics and double descent curve}},
  journal       = {arXiv e-prints},
  keywords      = {Mathematics - Statistics Theory, Statistics - Machine Learning, 62J99},
  year          = 2019,
  month         = aug,
  eid           = {arXiv:1908.05355},
  pages         = {arXiv:1908.05355},
  archiveprefix = {arXiv},
  eprint        = {1908.05355},
  primaryclass  = {math.ST},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv190805355M},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{Geiger_2020,
  doi       = {10.1088/1742-5468/ab633c},
  url       = {https://doi.org/10.1088/1742-5468/ab633c},
  year      = 2020,
  month     = {feb},
  publisher = {{IOP} Publishing},
  volume    = {2020},
  number    = {2},
  pages     = {023401},
  author    = {Mario Geiger and Arthur Jacot and Stefano Spigler and Franck Gabriel and Levent Sagun and St{\'{e}}phane d'Ascoli and Giulio Biroli and Cl{\'{e}}ment Hongler and Matthieu Wyart},
  title     = {Scaling description of generalization with number of parameters in deep learning},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  abstract  = {Supervised deep learning involves the training of neural networks with a large number N of parameters. For large enough N, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as N grows past a certain threshold N*. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with N. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations of the neural net output function f N around its expectation . These affect the generalization error for classification: under natural assumptions, it decays to a plateau value in a power-law fashion  ∼N−1/2. This description breaks down at a so-called jamming transition N  =  N*. At this threshold, we argue that diverges. This result leads to a plausible explanation for the cusp in test error known to occur at N*. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond N*, and averaging their outputs.}
}

@article{loureiro2021capturing,
  title={Learning curves of generic features maps for realistic datasets with a teacher-student model},
  author={Loureiro, Bruno and Gerbelot, Cedric and Cui, Hugo and Goldt, Sebastian and Krzakala, Florent and Mezard, Marc and Zdeborov{\'a}, Lenka},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18137--18151},
  year={2021}
}


@article{adlam2019random,
  author        = {{Adlam}, Ben and {Levinson}, Jake and {Pennington}, Jeffrey},
  title         = {{A Random Matrix Perspective on Mixtures of Nonlinearities for Deep Learning}},
  journal       = {arXiv e-prints},
  keywords      = {Statistics - Machine Learning, Computer Science - Machine Learning},
  year          = 2019,
  month         = dec,
  eid           = {arXiv:1912.00827},
  pages         = {arXiv:1912.00827},
  archiveprefix = {arXiv},
  eprint        = {1912.00827},
  primaryclass  = {stat.ML},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2019arXiv191200827A},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}


@inproceedings{pmlr-v119-jacot20a,
  title     = {Implicit Regularization of Random Feature Models},
  author    = {Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Clement and Gabriel, Franck},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  pages     = {4631--4640},
  year      = {2020},
  editor    = {Hal Daumé III and Aarti Singh},
  volume    = {119},
  series    = {Proceedings of Machine Learning Research},
  month     = {13--18 Jul},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v119/jacot20a/jacot20a.pdf},
  url       = { http://proceedings.mlr.press/v119/jacot20a.html },
  abstract  = {Random Features (RF) models are used as efficient parametric approximations of kernel methods. We investigate, by means of random matrix theory, the connection between Gaussian RF models and Kernel Ridge Regression (KRR). For a Gaussian RF model with $P$ features, $N$ data points, and a ridge $\lambda$, we show that the average (i.e. expected) RF predictor is close to a KRR predictor with an \emph{effective ridge} $\tilde{\lambda}$. We show that $\tilde{\lambda} > \lambda$ and $\tilde{\lambda} \searrow \lambda$ monotonically as $P$ grows, thus revealing the \emph{implicit regularization effect} of finite RF sampling. We then compare the risk (i.e. test error) of the $\tilde{\lambda}$-KRR predictor with the average risk of the $\lambda$-RF predictor and obtain a precise and explicit bound on their difference. Finally, we empirically find an extremely good agreement between the test errors of the average $\lambda$-RF predictor and $\tilde{\lambda}$-KRR predictor.}
} 

@article{10.1214/19-ECP262,
  author    = {S. Péché},
  title     = {{A note on the Pennington-Worah distribution}},
  volume    = {24},
  journal   = {Electronic Communications in Probability},
  number    = {none},
  publisher = {Institute of Mathematical Statistics and Bernoulli Society},
  pages     = {1 -- 7},
  keywords  = {machine learning, random matrices},
  year      = {2019},
  doi       = {10.1214/19-ECP262},
  url       = {https://doi.org/10.1214/19-ECP262}
}


@article{PhysRevLett.56.889,
  title = {Dynamic Scaling of Growing Interfaces},
  author = {Kardar, Mehran and Parisi, Giorgio and Zhang, Yi-Cheng},
  journal = {Phys. Rev. Lett.},
  volume = {56},
  issue = {9},
  pages = {889--892},
  numpages = {0},
  year = {1986},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.56.889},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.56.889}
}


@book{mehta2004random,
  title={Random matrices},
  author={Mehta, Madan Lal},
  year={2004},
  publisher={Elsevier}
}

@article{8180454,
  author  = {J. W. {Helton} and R. R. {Far} and R. {Speicher}},
  journal = {International Mathematics Research Notices},
  title   = {Operator-valued Semicircular Elements: Solving A Quadratic Matrix Equation with Positivity Constraints},
  year    = {2007},
  volume  = {2007},
  number  = {9},
  pages   = {rnm086-rnm086},
  doi     = {10.1093/imrn/rnm086}
}

@article{cui2020perturbative,
  title={The perturbative resolvent method: Spectral densities of random matrix ensembles via perturbation theory},
  author={Cui, Wenping and Rocks, Jason W and Mehta, Pankaj},
  journal={arXiv preprint arXiv:2012.00663},
  year={2020}
}

@book{mingo2017free,
  title     = {Free probability and random matrices},
  author    = {Mingo, James A and Speicher, Roland},
  volume    = {35},
  year      = {2017},
  publisher = {Springer}
}

@article{helton2018applications,
  title     = {Applications of realizations (aka linearizations) to free probability},
  author    = {Helton, J William and Mai, Tobias and Speicher, Roland},
  journal   = {Journal of Functional Analysis},
  volume    = {274},
  number    = {1},
  pages     = {1--79},
  year      = {2018},
  publisher = {Elsevier}
}


@inproceedings{dascoli2020triple,
  author    = {d{'}Ascoli, St\'{e}phane and Sagun, Levent and Biroli, Giulio},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
  pages     = {3058--3069},
  publisher = {Curran Associates, Inc.},
  title     = {Triple descent and the two kinds of overfitting: where and why do they appear?},
  url       = {https://proceedings.neurips.cc/paper/2020/file/1fd09c5f59a8ff35d499c0ee25a1d47e-Paper.pdf},
  volume    = {33},
  year      = {2020}
}

@inproceedings{nakkiran2019deep,
  title     = {Deep Double Descent: Where Bigger Models and More Data Hurt},
  author    = {Preetum Nakkiran and Gal Kaplun and Yamini Bansal and Tristan Yang and Boaz Barak and Ilya Sutskever},
  booktitle = {International Conference on Learning Representations},
  year      = {2020}
}

@inproceedings{RechtRahimi2007,
  author    = {Rahimi, Ali and Recht, Benjamin},
  booktitle = {Advances in Neural Information Processing Systems},
  editor    = {J. Platt and D. Koller and Y. Singer and S. Roweis},
  pages     = {},
  publisher = {Curran Associates, Inc.},
  title     = {Random Features for Large-Scale Kernel Machines},
  url       = {https://proceedings.neurips.cc/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf},
  volume    = {20},
  year      = {2008}
}

@inproceedings{pennington2017nonlinear,
  author    = {Jeffrey Pennington and
               Pratik Worah},
  editor    = {Isabelle Guyon and
               Ulrike von Luxburg and
               Samy Bengio and
               Hanna M. Wallach and
               Rob Fergus and
               S. V. N. Vishwanathan and
               Roman Garnett},
  title     = {Nonlinear random matrix theory for deep learning},
  booktitle = {Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, {USA}},
  pages     = {2637--2646},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/0f3d014eead934bbdbacb62a01dc4831-Abstract.html},
  timestamp = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl    = {https://dblp.org/rec/conf/nips/PenningtonW17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{hopfield1982neural,
  title={Neural networks and physical systems with emergent collective computational abilities.},
  author={Hopfield, John J},
  journal={Proceedings of the national academy of sciences},
  volume={79},
  number={8},
  pages={2554--2558},
  year={1982},
  publisher={National Acad Sciences}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{pmlr-v134-bodin21a,
  title     = {Rank-one matrix estimation: analytic time evolution of gradient descent dynamics},
  author    = {Bodin, Antoine and Macris, Nicolas},
  booktitle = {Proceedings of Thirty Fourth Conference on Learning Theory},
  pages     = {635--678},
  year      = {2021},
  editor    = {Belkin, Mikhail and Kpotufe, Samory},
  volume    = {134},
  series    = {Proceedings of Machine Learning Research},
  month     = {15--19 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v134/bodin21a/bodin21a.pdf},
  url       = {https://proceedings.mlr.press/v134/bodin21a.html},
  abstract  = {We consider a rank-one symmetric matrix corrupted by additive noise. The rank-one matrix is formed by an n-component unknown vector on the sphere of radius $\sqrt{n}$, and we consider the problem of estimating this vector from the corrupted matrix in the high dimensional limit of $n$ large, by gradient descent for a quadratic cost function on the sphere. Explicit formulas for the whole time evolution of the overlap between the estimator and unknown vector, as well as the cost, are rigorously derived. In the long time limit we recover the well known spectral phase transition, as a function of the signal-to-noise ratio. The explicit formulas also allow to point out interesting transient features of the time evolution. Our analysis technique is based on recent progress in random matrix theory and uses local versions of the semi-circle law.}
}

@article{rubio2011spectral,
  title     = {Spectral convergence for a general class of random matrices},
  author    = {Rubio, Francisco and Mestre, Xavier},
  journal   = {Statistics \& probability letters},
  volume    = {81},
  number    = {5},
  pages     = {592--602},
  year      = {2011},
  publisher = {Elsevier}
}

@book{GrobnerBasesBook,
  author    = {Cox, David A. and Little, John and O'Shea, Donal},
  title     = {Ideals, Varieties, and Algorithms: An Introduction to Computational Algebraic Geometry and Commutative Algebra, 3/e (Undergraduate Texts in Mathematics)},
  year      = {2007},
  isbn      = {0387356509},
  publisher = {Springer-Verlag},
  address   = {Berlin, Heidelberg}
}

@inbook{Opper1995,
  author    = {Opper, Manfred},
  title     = {Statistical Mechanics of Generalization},
  year      = {1998},
  isbn      = {0262511029},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  booktitle = {The Handbook of Brain Theory and Neural Networks},
  pages     = {922-925},
  numpages  = {4}
}

@book{engelvandenbroeck2001,
  place     = {Cambridge},
  title     = {Statistical Mechanics of Learning},
  doi       = {10.1017/CBO9781139164542},
  publisher = {Cambridge University Press},
  author    = {Engel, A. and Van den Broeck, C.},
  year      = {2001}
}

@article{Marcenko_1967,
doi = {10.1070/SM1967v001n04ABEH001994},
url = {https://dx.doi.org/10.1070/SM1967v001n04ABEH001994},
year = {1967},
month = {apr},
publisher = {},
volume = {1},
number = {4},
pages = {457},
author = {V A Marčenko and  L A Pastur},
title = {DISTRIBUTION OF EIGENVALUES FOR SOME SETS OF RANDOM MATRICES},
journal = {Mathematics of the USSR-Sbornik},
abstract = {}
}


@article{spectra,
  author        = {{Rashidi Far}, Reza and {Oraby}, Tamer and {Bryc}, Wlodzimierz and {Speicher}, Roland},
  title         = {{Spectra of large block matrices}},
  journal       = {arXiv e-prints},
  keywords      = {Computer Science - Information Theory, Mathematics - Operator Algebras, H.1.1},
  year          = 2006,
  month         = oct,
  eid           = {cs/0610045},
  pages         = {cs/0610045},
  archiveprefix = {arXiv},
  eprint        = {cs/0610045},
  primaryclass  = {cs.IT},
  adsurl        = {https://ui.adsabs.harvard.edu/abs/2006cs.......10045R},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@book{mezard1987spin,
  title     = {Spin Glass Theory and Beyond},
  author    = {Mezard, M. and Parisi, G. and Virasoro, M.A.},
  isbn      = {9789971501150},
  lccn      = {sls90082516},
  series    = {Lecture Notes in Physics Series},
  url       = {https://books.google.ch/books?id=ZIF9QgAACAAJ},
  year      = {1987},
  publisher = {World Scientific}
}


@article{hendrycks2021unsolved,
  title   = {Unsolved Problems in ML Safety},
  author  = {Hendrycks, Dan and Carlini, Nicholas and Schulman, John and Steinhardt, Jacob},
  journal = {arXiv preprint arXiv:2109.13916},
  year    = {2021}
}

@inproceedings{power2021grokking,
  title     = {GROKKING: GENERALIZATION BEYOND OVERFITTING ON SMALL ALGORITHMIC DATASETS},
  author    = {Power, Alethea and Burda, Yuri and Edwards, Harri and Babuschkin, Igor and Misra, Vedant},
  booktitle = {ICLR MATH-AI Workshop},
  year      = {2021}
}

@inproceedings{liao2018dynamics,
  title        = {The dynamics of learning: A random matrix approach},
  author       = {Liao, Zhenyu and Couillet, Romain},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3072--3081},
  year         = {2018},
  organization = {PMLR}
}



@article{WYART1,
  author     = {Mario Geiger and
                Arthur Jacot and
                Stefano Spigler and
                Franck Gabriel and
                Levent Sagun and
                St{\'{e}}phane d'Ascoli and
                Giulio Biroli and
                Cl{\'{e}}ment Hongler and
                Matthieu Wyart},
  title      = {Scaling description of generalization with number of parameters in
                deep learning},
  journal    = {CoRR},
  volume     = {abs/1901.01608},
  year       = {2019},
  url        = {http://arxiv.org/abs/1901.01608},
  eprinttype = {arXiv},
  eprint     = {1901.01608},
  timestamp  = {Sat, 23 Jan 2021 01:20:16 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1901-01608.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{WYART2,
  title     = {A jamming transition from under-to over-parametrization affects generalization in deep learning},
  author    = {Spigler, Stefano and Geiger, Mario and d’Ascoli, St{\'e}phane and Sagun, Levent and Biroli, Giulio and Wyart, Matthieu},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  volume    = {52},
  number    = {47},
  pages     = {474001},
  year      = {2019},
  publisher = {IOP Publishing}
}

@article{AdvaniSaxeSompolynski,
  title    = {High-dimensional dynamics of generalization error in neural networks},
  journal  = {Neural Networks},
  volume   = {132},
  pages    = {428-446},
  year     = {2020},
  issn     = {0893-6080},
  doi      = {https://doi.org/10.1016/j.neunet.2020.08.022},
  url      = {https://www.sciencedirect.com/science/article/pii/S0893608020303117},
  author   = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
  keywords = {Neural networks, Generalization error, Random matrix theory},
  abstract = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.}
}

@inproceedings{DoubleTrouble,
  title        = {Double trouble in double descent: Bias and variance (s) in the lazy regime},
  author       = {d’Ascoli, St{\'e}phane and Refinetti, Maria and Biroli, Giulio and Krzakala, Florent},
  booktitle    = {International Conference on Machine Learning},
  pages        = {2280--2290},
  year         = {2020},
  organization = {PMLR}
}

@inproceedings{Gerace-et-al,
  title        = {Generalisation error in learning with random features and the hidden manifold model},
  author       = {Gerace, Federica and Loureiro, Bruno and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle    = {International Conference on Machine Learning},
  pages        = {3452--3462},
  year         = {2020},
  organization = {PMLR}
}

@article{Deng2019,
  author   = {Deng, Zeyu and Kammoun, Abla and Thrampoulidis, Christos},
  title    = {{A model of double descent for high-dimensional binary linear classification}},
  journal  = {Information and Inference: A Journal of the IMA},
  volume   = {11},
  number   = {2},
  pages    = {435-495},
  year     = {2021},
  month    = {04},
  abstract = {{We consider a model for logistic regression where only a subset of features of size \\$p\\$ is used for training a linear classifier over \\$n\\$ training samples. The classifier is obtained by running gradient descent on logistic loss. For this model, we investigate the dependence of the classification error on the ratio \\$\\kappa =p/n\\$. First, building on known deterministic results on the implicit bias of gradient descent, we uncover a phase-transition phenomenon for the case of Gaussian features: the classification error of the gradient descent solution is the same as that of the maximum-likelihood solution when \\$\\kappa \\&lt;\\kappa \_\\star \\$, and that of the support vector machine when \\$\\kappa\\&gt;\\kappa \_\\star \\$, where \\$\\kappa \_\\star \\$ is a phase-transition threshold. Next, using the convex Gaussian min–max theorem, we sharply characterize the performance of both the maximum-likelihood and the support vector machine solutions. Combining these results, we obtain curves that explicitly characterize the classification error for varying values of \\$\\kappa \\$. The numerical results validate the theoretical predictions and unveil double-descent phenomena that complement similar recent findings in linear regression settings as well as empirical observations in more complex learning scenarios.}},
  issn     = {2049-8772},
  doi      = {10.1093/imaiai/iaab002},
  url      = {https://doi.org/10.1093/imaiai/iaab002},
  eprint   = {https://academic.oup.com/imaiai/article-pdf/11/2/435/44020681/iaab002.pdf}
}


@article{hu2022sharp,
  title={Sharp asymptotics of kernel ridge regression beyond the linear regime},
  author={Hu, Hong and Lu, Yue M},
  journal={arXiv preprint arXiv:2205.06798},
  year={2022}
}

@inproceedings{Kin-et-Thrampoulidis2020,
  author    = {Kini, Ganesh Ramachandra and Thrampoulidis, Christos},
  booktitle = {2020 IEEE International Symposium on Information Theory (ISIT)},
  title     = {Analytic Study of Double Descent in Binary Classification: The Impact of Loss},
  year      = {2020},
  volume    = {},
  number    = {},
  pages     = {2527-2532},
  doi       = {10.1109/ISIT44484.2020.9174344}
}

@book{livre-Engel-vandenBroeck,
  title     = {Statistical mechanics of learning},
  author    = {Engel, Andreas and Van den Broeck, Christian},
  year      = {2001},
  publisher = {Cambridge University Press}
}


@article{lu2022equivalence,
  title   = {An Equivalence Principle for the Spectrum of Random Inner-Product Kernel Matrices},
  author  = {Lu, Yue M and Yau, Horng-Tzer},
  journal = {arXiv preprint arXiv:2205.06308},
  year    = {2022}
}

@article{loog2020brief,
  title={A brief prehistory of double descent},
  author={Loog, Marco and Viering, Tom and Mey, Alexander and Krijthe, Jesse H and Tax, David MJ},
  journal={Proceedings of the National Academy of Sciences},
  volume={117},
  number={20},
  pages={10625--10626},
  year={2020},
  publisher={National Acad Sciences}
}

@article{belkin2019reconciling,
  title={Reconciling modern machine-learning practice and the classical bias--variance trade-off},
  author={Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  journal={Proceedings of the National Academy of Sciences},
  volume={116},
  number={32},
  pages={15849--15854},
  year={2019},
  publisher={National Acad Sciences}
}


@inproceedings{el2018estimation,
  title={Estimation in the spiked Wigner model: a short proof of the replica formula},
  author={El Alaoui, Ahmed and Krzakala, Florent},
  booktitle={2018 IEEE International Symposium on Information Theory (ISIT)},
  pages={1874--1878},
  year={2018},
  organization={IEEE}
}

@article{adlam2020understanding,
  title={Understanding double descent requires a fine-grained bias-variance decomposition},
  author={Adlam, Ben and Pennington, Jeffrey},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={11022--11032},
  year={2020}
}

@incollection{XXT,
 author = {barbier, jean and Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and Lesieur, Thibault and Zdeborov\'{a}, Lenka},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula},
 url = {https://proceedings.neurips.cc/paper_files/paper/2016/file/621bf66ddb7c962aa0d22ac97d69b793-Paper.pdf},
 volume = {29},
 year = {2016}
}


@article{2016arXiv161103888L,
  title     = {Fundamental limits of symmetric low-rank matrix estimation},
  author    = {Lelarge, Marc and Miolane, L{\'e}o},
  journal   = {Probability Theory and Related Fields},
  volume    = {173},
  number    = {3-4},
  pages     = {859--929},
  year      = {2018},
  publisher = {Springer}
}

@article{alex2014,
  author    = {Bloemendal, Alex and Erdős, László and Knowles, Antti and Yau, Horng-Tzer and Yin, Jun},
  doi       = {10.1214/EJP.v19-3054},
  fjournal  = {Electronic Journal of Probability},
  journal   = {Electron. J. Probab.},
  pages     = {53 pp.},
  pno       = {33},
  publisher = {The Institute of Mathematical Statistics and the Bernoulli Society},
  title     = {Isotropic local laws for sample covariance and generalized Wigner matrices},
  url       = {https://doi.org/10.1214/EJP.v19-3054},
  volume    = {19},
  year      = {2014}
}

@article{Erdoes_2008,
  title     = {Local Semicircle Law and Complete Delocalization for Wigner Random Matrices},
  volume    = {287},
  issn      = {1432-0916},
  url       = {http://dx.doi.org/10.1007/s00220-008-0636-9},
  doi       = {10.1007/s00220-008-0636-9},
  number    = {2},
  journal   = {Communications in Mathematical Physics},
  publisher = {Springer Science and Business Media LLC},
  author    = {Erd{\'o}s, L{\'a}szl{\'o} and Schlein, Benjamin and Yau, Horng-Tzer},
  year      = {2008},
  month     = {Sep},
  pages     = {641-655}
}

@book{vershynin_2018, 
  place={Cambridge}, 
  series={Cambridge Series in Statistical and Probabilistic Mathematics}, 
  title={High-Dimensional Probability: An Introduction with Applications in Data Science}, DOI={10.1017/9781108231596}, 
  publisher={Cambridge University Press}, author={Vershynin, Roman}, year={2018}, 
  collection={Cambridge Series in Statistical and Probabilistic Mathematics}
}

@unpublished{benaychgeorges:hal-01258444,
  title               = {{Lectures on the local semicircle law for Wigner matrices}},
  author              = {Benaych-Georges, Florent and Knowles, Antti},
  url                 = {https://hal.archives-ouvertes.fr/hal-01258444},
  note                = {working paper or preprint},
  hal_local_reference = {MAP5 2016-05},
  year                = {2016},
  keywords            = {Random matrices},
  hal_id              = {hal-01258444},
  hal_version         = {v1}
}

@article{FP2006,
  author  = {F{\'e}ral, Delphine and P{\'e}ch{\'e}, Sandrine},
  year    = {2006},
  month   = {06},
  pages   = {},
  title   = {The Largest Eigenvalue of Rank One Deformation of Large Wigner Matrices},
  volume  = {272},
  journal = {Communications in Mathematical Physics},
  doi     = {10.1007/s00220-007-0209-3}
}

@article{baik2005,
  author    = {Baik, Jinho and Ben Arous, G{\'e}rard and P{\'e}ch{\'e}, Sandrine},
  doi       = {10.1214/009117905000000233},
  fjournal  = {Annals of Probability},
  journal   = {Ann. Probab.},
  month     = {09},
  number    = {5},
  pages     = {1643--1697},
  publisher = {The Institute of Mathematical Statistics},
  title     = {Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  url       = {https://doi.org/10.1214/009117905000000233},
  volume    = {33},
  year      = {2005}
}


@article{misiakiewicz2022spectrum,
  title   = {Spectrum of inner-product kernel matrices in the polynomial regime and multiple descent phenomenon in kernel ridge regression},
  author  = {Misiakiewicz, Theodor},
  journal = {arXiv preprint arXiv:2204.10425},
  year    = {2022}
}

@inproceedings{Lecun98gradient-basedlearning,
  author    = {Yann Lecun and Léon Bottou and Yoshua Bengio and Patrick Haffner},
  title     = {Gradient-based learning applied to document recognition},
  booktitle = {Proceedings of the IEEE},
  year      = {1998},
  pages     = {2278--2324}
}

@article{xiao2022precise,
  title   = {Precise Learning Curves and Higher-Order Scaling Limits for Dot Product Kernel Regression},
  author  = {Xiao, Lechao and Pennington, Jeffrey},
  journal = {arXiv preprint arXiv:2205.14846},
  year    = {2022}
}


@article{dobriban2018high,
  title     = {High-dimensional asymptotics of prediction: Ridge regression and classification},
  author    = {Dobriban, Edgar and Wager, Stefan},
  journal   = {The Annals of Statistics},
  volume    = {46},
  number    = {1},
  pages     = {247--279},
  year      = {2018},
  publisher = {JSTOR}
}

@article{wu2020optimal,
  title   = {On the Optimal Weighted L2 Regularization in Overparameterized Linear Regression},
  author  = {Wu, Denny and Xu, Ji},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {33},
  pages   = {10112--10123},
  year    = {2020}
}

@inproceedings{richards2021asymptotics,
  title        = {Asymptotics of ridge (less) regression under general source condition},
  author       = {Richards, Dominic and Mourtada, Jaouad and Rosasco, Lorenzo},
  booktitle    = {International Conference on Artificial Intelligence and Statistics},
  pages        = {3889--3897},
  year         = {2021},
  organization = {PMLR}
}

@article{meng2022multiple,
  title   = {Multiple Descent in the Multiple Random Feature Model},
  author  = {Meng, Xuran and Yao, Jianfeng and Cao, Yuan},
  journal = {arXiv preprint arXiv:2208.09897},
  year    = {2022}
}

@inproceedings{goldt2022gaussian,
  title={The gaussian equivalence of generative models for learning with shallow neural networks},
  author={Goldt, Sebastian and Loureiro, Bruno and Reeves, Galen and Krzakala, Florent and M{\'e}zard, Marc and Zdeborov{\'a}, Lenka},
  booktitle={Mathematical and Scientific Machine Learning},
  pages={426--471},
  year={2022},
  organization={PMLR}
}

@article{10.7717/peerj-cs.103,
  title = {SymPy: symbolic computing in Python},
  author = {Meurer, Aaron and Smith, Christopher P. and Paprocki, Mateusz and \v{C}ert\'{i}k, Ond\v{r}ej and Kirpichev, Sergey B. and Rocklin, Matthew and Kumar, AMiT and Ivanov, Sergiu and Moore, Jason K. and Singh, Sartaj and Rathnayake, Thilina and Vig, Sean and Granger, Brian E. and Muller, Richard P. and Bonazzi, Francesco and Gupta, Harsh and Vats, Shivam and Johansson, Fredrik and Pedregosa, Fabian and Curry, Matthew J. and Terrel, Andy R. and Rou\v{c}ka, \v{S}t\v{e}p\'{a}n and Saboo, Ashutosh and Fernando, Isuru and Kulal, Sumith and Cimrman, Robert and Scopatz, Anthony},
  year = 2017,
  month = jan,
  keywords = {Python, Computer algebra system, Symbolics},
  abstract = {
            SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.
          },
  volume = 3,
  pages = {e103},
  journal = {PeerJ Computer Science},
  issn = {2376-5992},
  url = {https://doi.org/10.7717/peerj-cs.103},
  doi = {10.7717/peerj-cs.103}
}


@article{lin2021causes,
  title   = {What Causes the Test Error? Going Beyond Bias-Variance via ANOVA.},
  author  = {Lin, Licong and Dobriban, Edgar},
  journal = {J. Mach. Learn. Res.},
  volume  = {22},
  pages   = {155--1},
  year    = {2021}
}

@inproceedings{tarmoun2021understanding,
  title        = {Understanding the dynamics of gradient flow in overparameterized linear models},
  author       = {Tarmoun, Salma and Franca, Guilherme and Haeffele, Benjamin D and Vidal, Rene},
  booktitle    = {International Conference on Machine Learning},
  pages        = {10153--10161},
  year         = {2021},
  organization = {PMLR}
}

@article{gunasekar2017implicit,
  title   = {Implicit regularization in matrix factorization},
  author  = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {30},
  year    = {2017}
}


@inproceedings{jacot2020implicit,
  title={Implicit regularization of random feature models},
  author={Jacot, Arthur and Simsek, Berfin and Spadaro, Francesco and Hongler, Cl{\'e}ment and Gabriel, Franck},
  booktitle={International Conference on Machine Learning},
  pages={4631--4640},
  year={2020},
  organization={PMLR}
}


@article{guionnet2002large,
  title     = {Large deviations asymptotics for spherical integrals},
  author    = {Guionnet, Alice and Zeitouni, Ofer},
  journal   = {Journal of functional analysis},
  volume    = {188},
  number    = {2},
  pages     = {461--515},
  year      = {2002},
  publisher = {Elsevier}
}

@book{anderson2010introduction,
  title     = {An introduction to random matrices},
  author    = {Anderson, Greg W and Guionnet, Alice and Zeitouni, Ofer},
  year      = {2010},
  publisher = {Cambridge university press}
}

@article{davidson2001local,
  title   = {Local operator theory, random matrices and Banach spaces},
  author  = {Davidson, Kenneth R and Szarek, Stanislaw J},
  journal = {Handbook of the geometry of Banach spaces},
  volume  = {1},
  number  = {317-366},
  pages   = {131},
  year    = {2001}
}

@book{villani2021topics,
  title     = {Topics in optimal transportation},
  author    = {Villani, C{\'e}dric},
  volume    = {58},
  year      = {2021},
  publisher = {American Mathematical Soc.}
}

@article{guo2005mutual,
  title     = {Mutual information and minimum mean-square error in {G}aussian
               channels},
  author    = {Guo, Dongning and Shamai, Shlomo and Verd{\'u}, Sergio},
  journal   = {IEEE transactions on information theory},
  volume    = {51},
  number    = {4},
  pages     = {1261--1282},
  year      = {2005},
  publisher = {IEEE}
}

@article{matytsin1994large,
  title     = {On the large-{N} limit of the {I}tzykson-{Z}uber integral},
  author    = {Matytsin, A},
  journal   = {Nuclear Physics B},
  volume    = {411},
  number    = {2-3},
  pages     = {805--820},
  year      = {1994},
  publisher = {Elsevier}
}

@article{maillard2021perturbative,
  title     = {Perturbative construction of mean-field equations in
               extensive-rank matrix factorization and denoising},
  author    = {Maillard, Antoine and Krzakala, Florent and M{\'e}zard, Marc
               and Zdeborov{\'a}, Lenka},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment},
  volume    = {2022},
  number    = {8},
  pages     = {083301},
  year      = {2022},
  publisher = {IOP Publishing}
}

@article{biane1997free,
  title     = {On the free convolution with a semi-circular distribution},
  author    = {Biane, Philippe},
  journal   = {Indiana University Mathematics Journal},
  pages     = {705--718},
  year      = {1997},
  publisher = {JSTOR}
}

@article{verdu2010mismatched,
  title     = {Mismatched estimation and relative entropy},
  author    = {Verd{\'u}, Sergio},
  journal   = {IEEE Transactions on Information Theory},
  volume    = {56},
  number    = {8},
  pages     = {3712--3720},
  year      = {2010},
  publisher = {IEEE}
}

@article{voiculescu1993analogues,
  title     = {The analogues of entropy and of {F}isher's information measure
               in free probability theory, I},
  author    = {Voiculescu, Dan},
  journal   = {Communications in mathematical physics},
  volume    = {155},
  number    = {1},
  pages     = {71--92},
  year      = {1993},
  publisher = {Springer}
}

@article{voiculescu1994analogues,
  title     = {The analogues of entropy and of {F}isher's information measure
               in free probability theory, II},
  author    = {Voiculescu, Dan},
  journal   = {Inventiones mathematicae},
  volume    = {118},
  number    = {1},
  pages     = {411--440},
  year      = {1994},
  publisher = {Springer}
}

@article{yoshida2008remarks,
  title     = {Remarks on a semicircular perturbation of the free {F}isher
               information},
  author    = {Yoshida, Hiroaki},
  journal   = {Infinite Dimensional Analysis, Quantum Probability and
               Related Topics},
  volume    = {11},
  number    = {01},
  pages     = {97--108},
  year      = {2008},
  publisher = {World Scientific}
}


@article{voiculescu1997derivative,
  title     = {The derivative of order 1/2 of a free convolution by a
               semicircle distribution},
  author    = {Voiculescu, Dan},
  journal   = {Indiana University Mathematics Journal},
  pages     = {697--703},
  year      = {1997},
  publisher = {JSTOR}
}

@article{bun2016rotational,
  title     = {Rotational invariant estimator for general noisy matrices},
  author    = {Bun, Jo{\"e}l and Allez, Romain and Bouchaud, Jean-Philippe
               and Potters, Marc},
  journal   = {IEEE Transactions on Information Theory},
  volume    = {62},
  number    = {12},
  pages     = {7475--7490},
  year      = {2016},
  publisher = {IEEE}
}

@article{lelarge2019fundamental,
  title     = {Fundamental limits of symmetric low-rank matrix estimation},
  author    = {Lelarge, Marc and Miolane, L{\'e}o},
  journal   = {Probability Theory and Related Fields},
  volume    = {173},
  number    = {3},
  pages     = {859--929},
  year      = {2019},
  publisher = {Springer}
}

@article{dia2016mutual,
  title   = {Mutual information for symmetric rank-one matrix estimation: A
             proof of the replica formula},
  author  = {Dia, Mohamad and Macris, Nicolas and Krzakala, Florent and
             Lesieur, Thibault and Zdeborov{\'a}, Lenka and others},
  journal = {Advances in Neural Information Processing Systems},
  volume  = {29},
  year    = {2016}
}

@inproceedings{lesieur2015phase,
  title={Phase transitions in sparse PCA},
  author={Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  booktitle={2015 IEEE International Symposium on Information Theory (ISIT)},
  pages={1635--1639},
  year={2015},
  organization={IEEE}
}

@article{benaych2011eigenvalues,
  title     = {The eigenvalues and eigenvectors of finite, low rank
               perturbations of large random matrices},
  author    = {Benaych-Georges, Florent and Nadakuditi, Raj Rao},
  journal   = {Advances in Mathematics},
  volume    = {227},
  number    = {1},
  pages     = {494--521},
  year      = {2011},
  publisher = {Elsevier}
}

@article{janson2007resultant,
  title   = {Resultant and discriminant of polynomials},
  author  = {Janson, Svante},
  journal = {Notes, September},
  volume  = {22},
  year    = {2007}
}

@article{barbier2021statistical,
  title     = {Statistical limits of dictionary learning: random matrix
               theory and the spectral replica method},
  author    = {Barbier, Jean and Macris, Nicolas},
  journal   = {Physical Review E},
  volume    = {106},
  number    = {2},
  pages     = {024136},
  year      = {2022},
  publisher = {APS}
}

@article{miolane2017fundamental,
  title   = {Fundamental limits of low-rank matrix estimation: the
             non-symmetric case},
  author  = {Miolane, L{\'e}o},
  journal = {arXiv preprint arXiv:1702.00473},
  year    = {2017}
}

@article{korada2009exact,
  title     = {Exact solution of the gauge symmetric p-spin glass model on a
               complete graph},
  author    = {Korada, Satish Babu and Macris, Nicolas},
  journal   = {Journal of Statistical Physics},
  volume    = {136},
  number    = {2},
  pages     = {205--230},
  year      = {2009},
  publisher = {Springer}
}

@inproceedings{lesieur2015mmse,
  title        = {{MMSE} of probabilistic low-rank matrix estimation:
                  {U}niversality with respect to the output channel},
  author       = {Lesieur, Thibault and Krzakala, Florent and Zdeborov{\'a},
                  Lenka},
  booktitle    = {2015 53rd Annual Allerton Conference on Communication,
                  Control, and Computing (Allerton)},
  pages        = {680--687},
  year         = {2015},
  organization = {IEEE}
}

@article{barbier2019adaptive,
  title     = {The adaptive interpolation method: a simple scheme to prove
               replica formulas in {B}ayesian inference},
  author    = {Barbier, Jean and Macris, Nicolas},
  journal   = {Probability theory and related fields},
  volume    = {174},
  number    = {3},
  pages     = {1133--1185},
  year      = {2019},
  publisher = {Springer}
}

@article{barbier2019adaptive-b,
  title     = {The adaptive interpolation method for proving replica
               formulas. {A}pplications to the {C}urie--{W}eiss and {W}igner spike models},
  author    = {Barbier, Jean and Macris, Nicolas},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  volume    = {52},
  number    = {29},
  pages     = {294002},
  year      = {2019},
  publisher = {IOP Publishing}
}

@inproceedings{luneau2020high,
  title        = {High-dimensional rank-one nonsymmetric matrix decomposition:
                  the spherical case},
  author       = {Luneau, Cl{\'e}ment and Macris, Nicolas and Barbier, Jean},
  booktitle    = {2020 IEEE International Symposium on Information Theory
                  (ISIT)},
  pages        = {2646--2651},
  year         = {2020},
  organization = {IEEE}
}

@inproceedings{NEURIPS2022_f7e7fabd,
 author = {Ba, Jimmy and Erdogdu, Murat A and Suzuki, Taiji and Wang, Zhichao and Wu, Denny and Yang, Greg},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {S. Koyejo and S. Mohamed and A. Agarwal and D. Belgrave and K. Cho and A. Oh},
 pages = {37932--37946},
 publisher = {Curran Associates, Inc.},
 title = {High-dimensional Asymptotics of Feature Learning: How One Gradient Step Improves the Representation},
 url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/f7e7fabd73b3df96c54a320862afcb78-Paper-Conference.pdf},
 volume = {35},
 year = {2022}
}


@inproceedings{pourkamali2022mismatched,
  title        = {Mismatched Estimation of Non-Symmetric Rank-One Matrices Under
                  {G}aussian Noise},
  author       = {Pourkamali, Farzad and Macris, Nicolas},
  booktitle    = {2022 IEEE International Symposium on Information Theory
                  (ISIT)},
  pages        = {1288--1293},
  year         = {2022},
  organization = {IEEE}
}

@article{barbier2022price,
  title   = {The price of ignorance: how much does it cost to forget noise
             structure in low-rank matrix estimation?},
  author  = {Barbier, Jean and Hou, TianQi and Mondelli, Marco and
             S{\'a}enz, Manuel},
  journal = {arXiv preprint arXiv:2205.10009},
  year    = {2022}
}

@article{kabashima2016phase,
  title     = {Phase transitions and sample complexity in {B}ayes-optimal
               matrix factorization},
  author    = {Kabashima, Yoshiyuki and Krzakala, Florent and M{\'e}zard,
               Marc and Sakata, Ayaka and Zdeborov{\'a}, Lenka},
  journal   = {IEEE Transactions on information theory},
  volume    = {62},
  number    = {7},
  pages     = {4228--4265},
  year      = {2016},
  publisher = {IEEE}
}

@article{troiani2022optimal,
  title   = {Optimal denoising of rotationally invariant rectangular matrices},
  author  = {Troiani, Emanuele and Erba, Vittorio and Krzakala, Florent
             and Maillard, Antoine and Zdeborov{\'a}, Lenka},
  journal = {arXiv preprint arXiv:2203.07752},
  year    = {2022}
}

@article{husson2022spherical,
  title   = {Spherical Integrals of Sublinear Rank},
  author  = {Husson, Jonathan and Ko, Justin},
  journal = {arXiv preprint arXiv:2208.03642},
  year    = {2022}
}

@book{nica2006lectures,
  title     = {Lectures on the combinatorics of free probability},
  author    = {Nica, Alexandru and Speicher, Roland},
  volume    = {13},
  year      = {2006},
  publisher = {Cambridge University Press}
}


@inproceedings{muller2004random,
  title        = {Random matrices, free probability and the replica method},
  author       = {M{\"u}ller, Ralf R},
  booktitle    = {2004 12th European Signal Processing Conference},
  pages        = {189--196},
  year         = {2004},
  organization = {IEEE}
}

@inproceedings{reeves2018mutual,
  title        = {Mutual information as a function of matrix snr for linear
                  gaussian channels},
  author       = {Reeves, Galen and Pfister, Henry D and Dytso, Alex},
  booktitle    = {2018 IEEE International Symposium on Information Theory
                  (ISIT)},
  pages        = {1754--1758},
  year         = {2018},
  organization = {IEEE}
}

@article{dyson1962brownian,
  title={A Brownian-motion model for the eigenvalues of a random matrix},
  author={Dyson, Freeman J},
  journal={Journal of Mathematical Physics},
  volume={3},
  number={6},
  pages={1191--1198},
  year={1962},
  publisher={American Institute of Physics}
}


@inproceedings{nica2008free,
  title        = {Free probability extensions and applications},
  author       = {Nica, Alexandru and Speicher, Roland and Tulino, Antonia and
                  Voiculescu, Dan},
  booktitle    = {Proc. BIRS},
  pages        = {3--10},
  year         = {2008},
  organization = {Citeseer}
}

@book{tao2012topics,
  title={Topics in random matrix theory},
  author={Tao, Terence},
  volume={132},
  year={2012},
  publisher={American Mathematical Soc.}
}

@article{shlyakhtenko2020fractional,
  title   = {Fractional free convolution powers},
  author  = {Shlyakhtenko, Dimitri and Jekel, Terence Tao},
  journal = {arXiv preprint arXiv:2009.01882},
  year    = {2020}
}

@book{hardy1952inequalities,
  title     = {Inequalities},
  author    = {Hardy, Godfrey Harold and Littlewood, John Edensor and
               P{\'o}lya, George and P{\'o}lya, Gy{\"o}rgy and others},
  year      = {1952},
  publisher = {Cambridge university press}
}

@book{sauer2011numerical,
  title     = {Numerical analysis},
  author    = {Sauer, Timothy},
  year      = {2011},
  publisher = {Addison-Wesley Publishing Company}
}

@article{zuber2008large,
  title     = {The large-N limit of matrix integrals over the orthogonal group},
  author    = {Zuber, Jean-Bernard},
  journal   = {Journal of Physics A: Mathematical and Theoretical},
  volume    = {41},
  number    = {38},
  pages     = {382001},
  year      = {2008},
  publisher = {IOP Publishing}
}

@article{menon2017complex,
  title   = {The complex {B}urgers’ equation, the {HCIZ} integral and the
             {C}alogero-{M}oser system},
  author  = {Menon, Govind},
  journal = {preprint},
  year    = {2017}
}

@article{guionnet2004first,
  title     = {First order asymptotics of matrix integrals; a rigorous
               approach towards the understanding of matrix models},
  author    = {Guionnet, Alice},
  journal   = {Comm. Math. Phys.},
  volume    = {244},
  number    = {3},
  pages     = {527--569},
  year      = {2004},
  publisher = {Springer}
}

@article{jekel2020elementary,
  title     = {An elementary approach to free entropy theory for convex
               potentials},
  author    = {Jekel, David},
  journal   = {Anal.  PDE},
  volume    = {13},
  number    = {8},
  pages     = {2289--2374},
  year      = {2020},
  publisher = {Mathematical Sciences Publishers}
}

@inproceedings{mairal2009online,
  title     = {Online dictionary learning for sparse coding},
  author    = {Mairal, Julien and Bach, Francis and Ponce, Jean and Sapiro,
               Guillermo},
  booktitle = {Proceedings of the 26th annual international conference on
               machine learning},
  pages     = {689--696},
  year      = {2009}
}

@article{olshausen1996emergence,
  title     = {Emergence of simple-cell receptive field properties by
               learning a sparse code for natural images},
  author    = {Olshausen, Bruno A and Field, David J},
  journal   = {Nature},
  volume    = {381},
  number    = {6583},
  pages     = {607--609},
  year      = {1996},
  publisher = {Nature Publishing Group}
}

@article{olshausen1997sparse,
  title     = {Sparse coding with an overcomplete basis set: A strategy
               employed by {V}1?},
  author    = {Olshausen, Bruno A and Field, David J},
  journal   = {Vision research},
  volume    = {37},
  number    = {23},
  pages     = {3311--3325},
  year      = {1997},
  publisher = {Elsevier}
}

@article{belouchrani1997blind,
  title     = {A blind source separation technique using second-order
               statistics},
  author    = {Belouchrani, Adel and Abed-Meraim, Karim and Cardoso, J-F and
               Moulines, Eric},
  journal   = {IEEE Transactions on signal processing},
  volume    = {45},
  number    = {2},
  pages     = {434--444},
  year      = {1997},
  publisher = {IEEE}
}

@article{candes2011robust,
  title     = {Robust principal component analysis?},
  author    = {Cand{\`e}s, Emmanuel J and Li, Xiaodong and Ma, Yi and
               Wright, John},
  journal   = {Journal of the ACM (JACM)},
  volume    = {58},
  number    = {3},
  pages     = {1--37},
  year      = {2011},
  publisher = {ACM New York, NY, USA}
}

@phdthesis{miolane2019fundamental,
  title  = {Fundamental limits of inference: A statistical physics approach.},
  author = {Miolane, L{\'e}o},
  year   = {2019},
  school = {Ecole normale sup{\'e}rieure-ENS PARIS; Inria Paris}
}

@article{donoho2013phase,
  title     = {The phase transition of matrix recovery from Gaussian
               measurements matches the minimax MSE of matrix denoising},
  author    = {Donoho, David L and Gavish, Matan and Montanari, Andrea},
  journal   = {Proceedings of the National Academy of Sciences},
  volume    = {110},
  number    = {21},
  pages     = {8405--8410},
  year      = {2013},
  publisher = {National Acad Sciences}
}

@article{barbier2022bayes,
  title   = {Bayes-optimal limits in structured PCA, and how to reach them},
  author  = {Barbier, Jean and Camilli, Francesco and Mondelli, Marco and
             Saenz, Manuel},
  journal = {arXiv preprint arXiv:2210.01237},
  year    = {2022}
}

@inproceedings{pourkamali2021mismatched,
  title        = {Mismatched Estimation of Symmetric Rank-One Matrices Under
                  Gaussian Noise},
  author       = {Pourkamali, Farzad and Macris, Nicolas},
  booktitle    = {International Zurich Seminar on Information and
                  Communication (IZS 2022). Proceedings},
  pages        = {84--88},
  year         = {2022},
  organization = {ETH Zurich}
}

@phdthesis{schmidt2018statistical,
  title  = {Statistical Physics of Sparse and Dense Models in Optimization
            and Inference},
  author = {Schmidt, Hinnerk Christian},
  year   = {2018},
  school = {Universit{\'e} Paris Saclay (COmUE)}
}




@article{voiculescu1991limit,
  title     = {Limit laws for random matrices and free products},
  author    = {Voiculescu, Dan},
  journal   = {Inventiones mathematicae},
  volume    = {104},
  number    = {1},
  pages     = {201--220},
  year      = {1991},
  publisher = {Springer}
}

@article{voiculescu1999analogues,
  title     = {The analogues of entropy and of Fisher's information measure
               in free probability theory: VI. Liberation and mutual free information},
  author    = {Voiculescu, Dan},
  journal   = {Advances in Mathematics},
  volume    = {146},
  number    = {2},
  pages     = {101--166},
  year      = {1999},
  publisher = {Elsevier}
}

@article{pesme2021implicit,
  title={Implicit bias of sgd for diagonal linear networks: a provable benefit of stochasticity},
  author={Pesme, Scott and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={29218--29230},
  year={2021}
}

@INPROCEEDINGS{bodin10161669,
  author={Bodin, Antoine and Macris, Nicolas},
  booktitle={2023 IEEE Information Theory Workshop (ITW)}, 
  title={Gradient flow on extensive-rank positive semi-definite matrix denoising}, 
  year={2023},
  volume={},
  number={},
  pages={365-370},
  doi={10.1109/ITW55543.2023.10161669}}

@misc{bodin.2212.06757,
  doi       = {10.48550/ARXIV.2212.06757},
  url       = {https://arxiv.org/abs/2212.06757},
  author    = {Bodin, Antoine and Macris, Nicolas},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {Gradient flow in the gaussian covariate model: exact solution of learning curves and multiple descent structures},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{chou2020gradient,
  title   = {Gradient descent for deep matrix factorization: Dynamics and implicit bias towards low rank},
  author  = {Chou, Hung-Hsu and Gieshoff, Carsten and Maly, Johannes and Rauhut, Holger},
  journal = {arXiv preprint arXiv:2011.13772},
  year    = {2020}
}

@article{saxe2013exact,
  title   = {Exact solutions to the nonlinear dynamics of learning in deep linear neural networks},
  author  = {Saxe, Andrew M and McClelland, James L and Ganguli, Surya},
  journal = {arXiv preprint arXiv:1312.6120},
  year    = {2013}
}


@article{baik2005phase,
  title   = {Phase transition of the largest eigenvalue for nonnull complex sample covariance matrices},
  author  = {J. Baik and G. Ben Arous and S. P{\'e}ch{\'e}},
  journal = {Annals of Probability},
  pages   = {1643},
  year    = {2005}
}

@inproceedings{DBLP:conf/isit/LesieurMLKZ17,
  author    = {Thibault Lesieur and
               L{\'{e}}o Miolane and
               Marc Lelarge and
               Florent Krzakala and
               Lenka Zdeborov{\'{a}}},
  title     = {Statistical and computational phase transitions in spiked tensor estimation},
  booktitle = {2017 {IEEE} International Symposium on Information Theory, {ISIT}
               2017, Aachen, Germany, June 25-30, 2017},
  pages     = {511--515},
  publisher = {{IEEE}},
  year      = {2017},
  url       = {https://doi.org/10.1109/ISIT.2017.8006580},
  doi       = {10.1109/ISIT.2017.8006580},
  timestamp = {Wed, 16 Oct 2019 14:14:48 +0200},
  biburl    = {https://dblp.org/rec/conf/isit/LesieurMLKZ17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{Pch2004TheLE,
  title   = {The largest eigenvalue of small rank perturbations of Hermitian random matrices},
  author  = {P{\'e}ch{\'e}, Sandrine},
  journal = {Probability Theory and Related Fields},
  year    = {2004},
  volume  = {134},
  pages   = {127-173}
}


@book{schwartz1958linear,
  title     = {Linear Operators},
  author    = {Dunford, N and Schwartz, J T},
  publisher = {Wiley  Classics Library},
  year      = {1988}
}

@article{chi2019nonconvex,
  title     = {Nonconvex optimization meets low-rank matrix factorization: An overview},
  author    = {Chi, Yuejie and Lu, Yue M and Chen, Yuxin},
  journal   = {IEEE Transactions on Signal Processing},
  volume    = {67},
  number    = {20},
  pages     = {5239--5269},
  year      = {2019},
  publisher = {IEEE}
}

@article{marchenko1967distribution,
  title     = {Distribution of eigenvalues for some sets of random matrices},
  author    = {Marchenko, Vladimir Alexandrovich and Pastur, Leonid Andreevich},
  journal   = {Matematicheskii Sbornik},
  volume    = {114},
  number    = {4},
  pages     = {507--536},
  year      = {1967},
  publisher = {Russian Academy of Sciences, Steklov Mathematical Institute of Russian~…}
}


@article{camilli2022matrix,
  title   = {Matrix factorization with neural networks},
  author  = {Camilli, Francesco and M{\'e}zard, Marc},
  journal = {arXiv preprint arXiv:2212.02105},
  year    = {2022}
}

@article{ChenChi,
  author  = {Chen, Yudong and Chi, Yuejie},
  journal = {IEEE Signal Processing Magazine},
  title   = {Harnessing Structures in Big Data via Guaranteed Low-Rank Matrix Estimation: Recent Theory and Fast Algorithms via Convex and Nonconvex Optimization},
  year    = {2018},
  volume  = {35},
  number  = {4},
  pages   = {14-31},
  doi     = {10.1109/MSP.2018.2821706}
}
  
  @article{Lesieur_2017,
  doi       = {10.1088/1742-5468/aa7284},
  url       = {https://dx.doi.org/10.1088/1742-5468/aa7284},
  year      = {2017},
  month     = {jul},
  publisher = {IOP Publishing and SISSA},
  volume    = {2017},
  number    = {7},
  pages     = {073403},
  author    = {Thibault Lesieur and Florent Krzakala and Lenka Zdeborová},
  title     = {Constrained low-rank matrix estimation: phase transitions, approximate message passing and applications},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment}
}

@inproceedings{Montanari-Richard-2014,
  author    = {Montanari, Andrea and Richard, Emile},
  title     = {A Statistical Model for Tensor PCA},
  year      = {2014},
  publisher = {MIT Press},
  address   = {Cambridge, MA, USA},
  booktitle = {Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2},
  pages     = {2897-2905},
  numpages  = {9},
  location  = {Montreal, Canada},
  series    = {NIPS 2014}
}

  @article{Montanari2017EstimationOL,
  title   = {Estimation of low-rank matrices via approximate message passing},
  author  = {Andrea Montanari and Ramji Venkataramanan},
  journal = {The Annals of Statistics},
  year    = {2017}
}

@article{Kabashima_2016,
  doi       = {10.1109/tit.2016.2556702},
  url       = {https://doi.org/10.1109%2Ftit.2016.2556702},
  year      = 2016,
  month     = {jul},
  publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  volume    = {62},
  number    = {7},
  pages     = {4228--4265},
  author    = {Yoshiyuki Kabashima and Florent Krzakala and Marc Mezard and Ayaka Sakata and Lenka Zdeborova},
  title     = {Phase Transitions and Sample Complexity in Bayes-Optimal Matrix Factorization},
  journal   = {{IEEE} Transactions on Information Theory}
}

@article{CamilliContucciMingione,
  title     = {{An inference problem in a mismatched setting: a spin-glass model with  Mattis interaction}},
  author    = {Francesco Camilli and Pierluigi Contucci and Emanuele Mingione},
  journal   = {SciPost Phys.},
  volume    = {12},
  pages     = {125},
  year      = {2022},
  publisher = {SciPost},
  doi       = {10.21468/SciPostPhys.12.4.125},
  url       = {https://scipost.org/10.21468/SciPostPhys.12.4.125}
}

@misc{BenArous-et-al-2022,
  doi       = {10.48550/ARXIV.2206.04030},
  url       = {https://arxiv.org/abs/2206.04030},
  author    = {Arous, Gerard Ben and Gheissari, Reza and Jagannath, Aukosh},
  keywords  = {Machine Learning (stat.ML), Machine Learning (cs.LG), Probability (math.PR), Statistics Theory (math.ST), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  title     = {High-dimensional limit theorems for SGD: Effective dynamics and critical scaling},
  publisher = {arXiv},
  year      = {2022},
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Liang-Sen-Sur-2022,
  author    = {Liang, Tengyuan and Sen, Subhabrata and Sur, Pragya},
  keywords  = {Statistics Theory (math.ST), Machine Learning (cs.LG), Probability (math.PR), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  title     = {High-dimensional Asymptotics of Langevin Dynamics in Spiked Matrix Models},
  publisher = {arXiv},
  year      = {2022},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{wigner1958distribution,
 ISSN = {0003486X},
 URL = {http://www.jstor.org/stable/1970008},
 author = {Eugene P. Wigner},
 journal = {Annals of Mathematics},
 number = {2},
 pages = {325--327},
 publisher = {Annals of Mathematics},
 title = {On the Distribution of the Roots of Certain Symmetric Matrices},
 urldate = {2023-10-09},
 volume = {67},
 year = {1958}
}




@article{ReevesP16,
  author = {Galen Reeves and
			  Henry D. Pfister},
  title  = {The Replica-Symmetric Prediction for Compressed Sensing with Gaussian
			  Matrices is Exact},
  year   = {2016},
  url    = {http://arxiv.org/abs/1607.02524}
}

@inproceedings{2017arXiv170910368B,
  author        = {{Barbier}, J. and {Macris}, N. and {Miolane}, L.},
  title         = {{The Layered Structure of Tensor Estimation and its
                  Mutual Information}},
  booktitle     = {55th Annual Allerton Conference on Communication, Control, and Computing (Allerton)},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1709.10368},
  month         = sep,
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170910368B},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2017arXiv170200473M,
  author        = {{Miolane}, L.},
  title         = {{Fundamental limits of low-rank matrix estimation: The non-symmetric case}},
  journal       = {ArXiv e-prints},
  archiveprefix = {arXiv},
  eprint        = {1702.00473},
  primaryclass  = {math.PR},
  keywords      = {Mathematics - Probability},
  year          = 2017,
  month         = feb,
  adsurl        = {http://adsabs.harvard.edu/abs/2017arXiv170200473M},
  adsnote       = {Provided by the SAO/NASA Astrophysics Data System}
}

@article{2017arXiv170100858L,
  doi       = {10.1088/1742-5468/aa7284},
  url       = {https://doi.org/10.1088%2F1742-5468%2Faa7284},
  year      = 2017,
  month     = {jul},
  publisher = {{IOP} Publishing},
  volume    = {2017},
  number    = {7},
  pages     = {073403},
  author    = {Thibault Lesieur and Florent Krzakala and Lenka Zdeborov{\'{a}}},
  title     = {Constrained low-rank matrix estimation: phase transitions, approximate message passing and applications},
  journal   = {Journal of Statistical Mechanics: Theory and Experiment}
}


@article{10.1214/19-AIHP960,
author = {Amelia Perry and Alexander S. Wein and Afonso S. Bandeira},
title = {{Statistical limits of spiked tensor models}},
volume = {56},
journal = {Annales de l'Institut Henri Poincar{\'e}, Probabilit{\'e}es et Statistiques},
number = {1},
publisher = {Institut Henri Poincar{\'e}},
pages = {230 - 264},
keywords = {contiguity, Second moment method, Spiked tensor model},
year = {2020},
doi = {10.1214/19-AIHP960},
URL = {https://doi.org/10.1214/19-AIHP960}
}



@article{Erdoes2011Survey,
  title     = {Universality of Wigner random matrices: a survey of recent results},
  volume    = {66},
  issn      = {1468-4829},
  url       = {http://dx.doi.org/10.1070/RM2011v066n03ABEH004749},
  doi       = {10.1070/rm2011v066n03abeh004749},
  number    = {3},
  journal   = {Russian Mathematical Surveys},
  publisher = {IOP Publishing},
  author    = {Erd{\'o}s, Laszl{\'o}},
  year      = {2011},
  month     = {Jun},
  pages     = {507-626}
}



@article{Cugliandolo1995,
  title     = {Full dynamical solution for a spherical spin-glass model},
  volume    = {28},
  issn      = {1361-6447},
  url       = {http://dx.doi.org/10.1088/0305-4470/28/15/003},
  doi       = {10.1088/0305-4470/28/15/003},
  number    = {15},
  journal   = {Journal of Physics A: Mathematical and General},
  publisher = {IOP Publishing},
  author    = {Cugliandolo, L F and Dean, D S},
  year      = {1995},
  month     = {Aug},
  pages     = {4213-4234}
}


@article{Cugliandolo1994,
  doi       = {10.1088/0305-4470/27/17/011},
  url       = {https://doi.org/10.1088%2F0305-4470%2F27%2F17%2F011},
  year      = 1994,
  month     = {sep},
  publisher = {{IOP} Publishing},
  volume    = {27},
  number    = {17},
  pages     = {5749--5772},
  author    = {L F Cugliandolo and J Kurchan},
  title     = {On the out-of-equilibrium relaxation of the Sherrington-Kirkpatrick model},
  journal   = {Journal of Physics A: Mathematical and General},
  abstract  = {Starting from a set of assumptions on the long-time limit behaviour of the nonequilibrium relaxation of mean-field models in the thermodynamic limit, we derive analytical results for the long-time relaxation of the Sherrington-Kirkpatrick model, starting from a random configuration. The system never achieves local equilibrium in any fixed sector of phase space, but remains in an asymptotic out-of-equilibrium regime. We clearly state and motivate the assumptions made. For the study of the out-of-equilibrium dynamics of spin-glass models, we propose as a tool, both numerical and analytical, the use of 'triangle relations' which describe the geometry of the configurations at three (long) different times.}
}

@article{wang2017scaling,
  title={Scaling limit: Exact and tractable analysis of online learning algorithms with applications to regularized regression and PCA},
  author={Wang, Chuang and Mattingly, Jonathan and Lu, Yue M},
  journal={arXiv preprint arXiv:1712.04332},
  year={2017}
}

@article{mei2018mean,
  title={A mean field view of the landscape of two-layer neural networks},
  author={Mei, Song and Montanari, Andrea and Nguyen, Phan-Minh},
  journal={Proceedings of the National Academy of Sciences},
  volume={115},
  number={33},
  pages={E7665--E7671},
  year={2018},
  publisher={National Acad Sciences}
}

@article{PhysRevB.25.6860,
  title     = {Relaxational dynamics of the Edwards-Anderson model and the mean-field theory of spin-glasses},
  author    = {Sompolinsky, H. and Zippelius, Annette},
  journal   = {Phys. Rev. B},
  volume    = {25},
  issue     = {11},
  pages     = {6860--6875},
  numpages  = {0},
  year      = {1982},
  month     = {Jun},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevB.25.6860},
  url       = {https://link.aps.org/doi/10.1103/PhysRevB.25.6860}
}



@article{PhysRevLett.47.359,
  title     = {Dynamic Theory of the Spin-Glass Phase},
  author    = {Sompolinsky, H. and Zippelius, Annette},
  journal   = {Phys. Rev. Lett.},
  volume    = {47},
  issue     = {5},
  pages     = {359--362},
  numpages  = {0},
  year      = {1981},
  month     = {Aug},
  publisher = {American Physical Society},
  doi       = {10.1103/PhysRevLett.47.359},
  url       = {https://link.aps.org/doi/10.1103/PhysRevLett.47.359}
}



@inproceedings{10.5555/3294996.3295123,
  author    = {Ge, Rong and Ma, Tengyu},
  title     = {On the Optimization Landscape of Tensor Decompositions},
  year      = {2017},
  isbn      = {9781510860964},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that "all local optima are (approximately) global optima", and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult.In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant ? > 0, among the set of points with function values (1 + ?)-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem.Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local optima of a highly-structured random polynomial with dependent coefficients.},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  pages     = {3656-3666},
  numpages  = {11},
  location  = {Long Beach, California, USA},
  series    = {NIPS'17}
}

  
@inproceedings{pmlr-v49-bandeira16,
  title     = {On the low-rank approach for semidefinite programs arising in synchronization and community detection},
  author    = {Afonso S. Bandeira and Nicolas Boumal and Vladislav Voroninski},
  pages     = {361--382},
  year      = {2016},
  editor    = {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
  volume    = {49},
  series    = {Proceedings of Machine Learning Research},
  address   = {Columbia University, New York, New York, USA},
  month     = {23--26 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v49/bandeira16.pdf},
  url       = {http://proceedings.mlr.press/v49/bandeira16.html},
  abstract  = {To address difficult optimization problems, convex relaxations based on semidefinite programming are now common place in many fields. Although solvable in polynomial time, large semidefinite programs tend to be computationally challenging. Over a decade ago, exploiting the fact that in many applications of interest the desired solutions are low rank, Burer and Monteiro proposed a heuristic to solve such semidefinite programs by restricting the search space to low-rank matrices. The accompanying theory does not explain the extent of the empirical success. We focus on Synchronization and Community Detection problems and provide theoretical guarantees shedding light on the remarkable efficiency of this heuristic.}
}

@article{ling2019landscape,
  title   = {On the Landscape of Synchronization Networks: A Perspective from Nonconvex Optimization},
  author  = {Shuyang Ling and Ruitu Xu and Afonso S. Bandeira},
  year    = {2019},
  journal = {arXiv:1809.11083}
}


@inproceedings{pmlr-v54-park17a,
  title     = {{Non-square matrix sensing without spurious local minima via the Burer-Monteiro approach}},
  author    = {Dohyung Park and Anastasios Kyrillidis and Constantine Carmanis and Sujay Sanghavi},
  pages     = {65--74},
  year      = {2017},
  editor    = {Aarti Singh and Jerry Zhu},
  volume    = {54},
  series    = {Proceedings of Machine Learning Research},
  address   = {Fort Lauderdale, FL, USA},
  month     = {20--22 Apr},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v54/park17a/park17a.pdf},
  url       = {http://proceedings.mlr.press/v54/park17a.html},
  abstract  = {We consider the non-square matrix sensing problem, under restricted isometry property (RIP) assumptions.  We focus on the non-convex formulation, where any rank-r matrix $X ∈R^m x n$ is represented as $UV^T$, where $U ∈R^m x r$ and $V ∈R^n x r$. In this paper, we complement recent findings on the non-convex geometry of the analogous PSD setting [5], and show that matrix factorization does not introduce any spurious local minima, under RIP. }
}


@inproceedings{10.5555/3045118.3045366,
  author    = {De Sa, Christopher and Olukotun, Kunle and R\'{e}, Christopher},
  title     = {Global Convergence of Stochastic Gradient Descent for Some Non-Convex Matrix Problems},
  year      = {2015},
  publisher = {JMLR.org},
  abstract  = {Stochastic gradient descent (SGD) on a low-rank factorization (Burer & Monteiro, 2003) is commonly employed to speed up matrix problems including matrix completion, subspace tracking, and SDP relaxation. In this paper, we exhibit a step size scheme for SGD on a low-rank least-squares problem, and we prove that, under broad sampling conditions, our method converges globally from a random starting point within O(?-1nlog n) steps with constant probability for constant-rank problems. Our modification of SGD relates it to stochastic power iteration. We also show experiments to illustrate the runtime and convergence of the algorithm.},
  booktitle = {Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37},
  pages     = {2332-2341},
  numpages  = {10},
  location  = {Lille, France},
  series    = {ICML'15}
}

@article{10.1214/20-AOS1958,
author = {Andrea Montanari and Ramji Venkataramanan},
title = {{Estimation of low-rank matrices via approximate message passing}},
volume = {49},
journal = {The Annals of Statistics},
number = {1},
publisher = {Institute of Mathematical Statistics},
pages = {321 - 345},
keywords = {approximate message passing, low-rank matrix estimation, spectral initialization},
year = {2021},
doi = {10.1214/20-AOS1958},
URL = {https://doi.org/10.1214/20-AOS1958}
}


@article{BurerMonteiroFirst,
  author  = {Burer, Samuel and Monteiro, Renato},
  year    = {2005},
  month   = {07},
  pages   = {427-444},
  title   = {Local Minima and Convergence in Low-Rank Semidefinite Programming},
  volume  = {103},
  journal = {Mathematical Programming},
  doi     = {10.1007/s10107-004-0564-1}
}

@article{BurerMonteiroSecond,
  author  = {Burer, Samuel and Monteiro, Renato},
  year    = {2003},
  month   = {02},
  pages   = {329-357},
  title   = {A Nonlinear Programming Algorithm for Solving Semidefinite Programs via Low-rank Factorization},
  volume  = {95},
  journal = {Mathematical Programming, Series B},
  doi     = {10.1007/s10107-002-0352-8}
}


@inproceedings{pmlr-v70-ge17a,
  title     = {No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis},
  author    = {Rong Ge and Chi Jin and Yi Zheng},
  pages     = {1233--1242},
  year      = {2017},
  editor    = {Doina Precup and Yee Whye Teh},
  volume    = {70},
  series    = {Proceedings of Machine Learning Research},
  address   = {International Convention Centre, Sydney, Australia},
  month     = {06--11 Aug},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v70/ge17a/ge17a.pdf},
  url       = {http://proceedings.mlr.press/v70/ge17a.html},
  abstract  = {In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.}
}



@inproceedings{10.5555/3157382.3157431,
  author    = {Ge, Rong and Lee, Jason D. and Ma, Tengyu},
  title     = {Matrix Completion Has No Spurious Local Minimum},
  year      = {2016},
  isbn      = {9781510838819},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for positive semidefinite matrix completion has no spurious local minima - all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve positive semidefinite matrix completion with arbitrary initialization in polynomial time. The result can be generalized to the setting when the observed entries contain noise. We believe that our main proof strategy can be useful for understanding geometric properties of other statistical problems involving partial or noisy observations.},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages     = {2981-2989},
  numpages  = {9},
  location  = {Barcelona, Spain},
  series    = {NIPS'16}
}


@inproceedings{pmlr-v49-lee16,
  title     = {Gradient Descent Only Converges to Minimizers},
  author    = {Jason D. Lee and Max Simchowitz and Michael I. Jordan and Benjamin Recht},
  pages     = {1246-1257},
  year      = {2016},
  editor    = {Vitaly Feldman and Alexander Rakhlin and Ohad Shamir},
  volume    = {49},
  series    = {Proceedings of Machine Learning Research},
  address   = {Columbia University, New York, New York, USA},
  month     = {23-pmlr-v49-lee1626 Jun},
  publisher = {PMLR},
  pdf       = {http://proceedings.mlr.press/v49/lee16.pdf},
  url       = {http://proceedings.mlr.press/v49/lee16.html},
  abstract  = {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.}
}





@inproceedings{10.5555/3305381.3305509,
  author    = {Ge, Rong and Jin, Chi and Zheng, Yi},
  title     = {No Spurious Local Minima in Nonconvex Low Rank Problems: A Unified Geometric Analysis},
  year      = {2017},
  publisher = {JMLR.org},
  abstract  = {In this paper we develop a new framework that captures the common landscape underlying the common non-convex low-rank matrix problems including matrix sensing, matrix completion and robust PCA. In particular, we show for all above problems (including asymmetric cases): 1) all local minima are also globally optimal; 2) no high-order saddle points exists. These results explain why simple algorithms such as stochastic gradient descent have global converge, and efficiently optimize these non-convex objective functions in practice. Our framework connects and simplifies the existing analyses on optimization landscapes for matrix sensing and symmetric matrix completion. The framework naturally leads to new results for asymmetric matrix completion and robust PCA.},
  booktitle = {Proceedings of the 34th International Conference on Machine Learning},
  pages     = {1233-1242},
  numpages  = {10},
  location  = {Sydney, NSW, Australia},
  series    = {ICML'17}
}



@inproceedings{10.5555/3157382.3157531,
  author    = {Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nathan},
  title     = {Global Optimality of Local Search for Low Rank Matrix Recovery},
  year      = {2016},
  isbn      = {9781510838819},
  publisher = {Curran Associates Inc.},
  address   = {Red Hook, NY, USA},
  abstract  = {We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent from random initialization.},
  booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
  pages     = {3880-3888},
  numpages  = {9},
  location  = {Barcelona, Spain},
  series    = {NIPS'16}
}


@article{Chi_2019,
  title   = {Nonconvex Optimization Meets Low-Rank Matrix Factorization: An Overview},
  volume  = {67},
  number  = {20},
  journal = {IEEE Transactions on Signal Processing},
  author  = {Chi, Yuejie and Lu, Yue M. and Chen, Yuxin},
  year    = {2019},
  month   = {Oct},
  pages   = {5239-5269}
}
