\documentclass[10pt]{beamer}

\usetheme{metropolis}
\usepackage{tcolorbox}


\usepackage{appendixnumberbeamer}

\usepackage{booktabs}
\usepackage[scale=2]{ccicons}

\usepackage{pgfplots}

\usepackage{xspace}
\newcommand{\themename}{\textbf{\textsc{metropolis}}\xspace}

\input{settings_custom.tex}

\definecolor{grey}{rgb}{0.5, 0.5, 0.5}
\tikzset{%
  every neuron/.style={
    circle,
    draw,
    minimum size=0.3cm
  },
  neuron missing/.style={
    draw=none, 
    scale=2,
    text height=0.333cm,
    execute at begin node=\color{black}$\vdots$
  },
}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes.symbols}


\usepgfplotslibrary{dateplot}


\metroset{block=fill}
\useinnertheme[showtitle=false]{tcolorbox}

\title{Random matrix methods for high-dimensional\\ machine learning models}
\subtitle{A statistical limit theory for ML models}
\date{\today}
\author{Antoine Bodin}
\institute{EPFL}
% \titlegraphic{\hfill\includegraphics[height=1.5cm]{logo.pdf}}



\begin{document}

\maketitle

\begin{frame}{Table of contents}
  \setbeamertemplate{section in toc}[sections numbered]
  \tableofcontents[hideallsubsections]
\end{frame}

\section{Introduction \& motivational example}


\begin{frame}{Why Random Matrix theory needed?}
  \begin{alertblock}{}
    \textbf{Random matrices} are ubuquitous in machine learning theory.
    \textbf{But} using them is not always straightforward.
    \textbf{So} can we develop simple tools to analyze high-dimensional models involving random matrices?
  \end{alertblock}
  \emph{Example: linear-model}
  \begin{itemize}
    \item \textbf{Data:} $X \in \mathbb R^{n \times d}$ with $X_{ij} \sim \mathcal N(0, \frac{1}{d})$ ($n$ samples, $d$ features), 
    \item \textbf{Output:} $Y = X \beta^* + \xi$ with $\xi \sim \mathcal N(0, \sigma^2 I_n)$ and $ \frac{1}{d}\norm{\beta^*}^2 = r^2$
    \item \textbf{Model:} $\hat \beta = (X^T X + \lambda I_d)^{-1} X^T Y$ (ridge regression)
  \end{itemize}

  % Question
  
  \vspace{0.4cm}
  \begin{alertblock}{Generalization error}
    \emph{Averaging over new random data points ($x$ and $y=x\beta^* + \epsilon$)}
  \begin{equation*}
    \Egen(\hat \beta)  = \mathbb E_{\epsilon, x} \left[ ( x^T \beta^* + \epsilon - x^T \hat \beta )^2 \right] 
    = \sigma^2 + \frac{1}{d} \norm{\beta^* - \hat \beta}^2
  \end{equation*}
  \end{alertblock}

\end{frame}

\begin{frame}{The High-dimensional Limit}
  Average $\mean_{X,Y} \Egen(\hat \beta)$ can be decomposed with Bias-variance:
\begin{align}
  \frac{1}{d} \mean_{X,\xi} \norm{\beta^*-\hat \beta}^2 = \mathcal B_X(\hat \beta) +  \mathcal V_X(\hat \beta)
\end{align}
with:
\begin{equation*}
  \mathcal B_X(\hat \beta) = \frac{1}{d} \mean\left[ \norm{ \beta^* - \mean[\hat \beta|X]}^2 \right]
  \qquad \text{and} \qquad
  \mathcal V_X(\hat \beta) = \frac{1}{d}  \mean\left[  \norm{ \hat \beta - \mean[\hat \beta|X]}^2 \right]
\end{equation*}
\emph{... After reducing everything with $ \frac{1}{d}\norm{\beta^*}^2 = r^2$:}
\begin{align}\label{eq:simple-bias}
  \mathcal B_X(\hat \beta) 
  & = \lambda^2 r^2 \mean_X \left\{ \frac{1}{d} \trace{ (X^T X + \lambda I)^{-2} } \right\}\\
  \mathcal V_X(\hat \beta)
  & = \sigma^2  \mean_X \left\{ \frac{1}{d} \trace{ (X^T X + \lambda I)^{-1}} - \lambda \frac{1}{d}\trace{(X^T X + \lambda I)^{-2}} \right\}
\end{align}
\begin{alertblock}{Conclusion}
  \begin{itemize}
    \item Both terms depend on the trace
    $\frac{1}{d}\trace{(X^T X + \lambda I)^{-1}}= g_d(-\lambda) $
    \item Simplifies in the high-dimensional limit $d \to \infty$.
  \end{itemize}
\end{alertblock}
\end{frame}

\begin{frame}{Random matrix theory: Marchenko-Pastur law}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      High-dimensional limit with $\phi = \frac{n}{d}$:
      $$
        g(z) = \lim_{d \to \infty} \frac{1}{d}  \mean \trace{(X^TX-zI)^{-1}}
      $$
      \vspace*{0.2cm}

      Then \cite{marchenko1967distribution}:
      $$
        z g^2 + (1+z - \phi) g +1 = 0
      $$

      Then also the derivative w.r.t. $z$:
      $$
      z (2gg' + g^2) + (1+z - \phi) g' + g = 0
      $$
    \end{column}
    \begin{column}{0.5\textwidth}
      % image marchenko-pastur.png with caption "\phi = 2"
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{images/marchenko-pastur.white.png}
        \caption{Marchenko-Pastur law with $\phi = 2, n=2000, d=1000$ and $\rho(\lambda) = \frac{1}{\pi} \Im g(\lambda + i 0^+)$}
      \end{figure}
    \end{column}
  \end{columns}
  \begin{alertblock}{Conclusion \cite{belkin2020two,Hastie-Montanari-2019}}
  $$
  \Egen(r,\sigma,\phi,\lambda) = \sigma^2 + \lambda^2 r^2 g'(-\lambda) + \sigma^2 (g(-\lambda) - \lambda g'(-\lambda))
  $$
  \end{alertblock}
\end{frame}

\begin{frame}{Double-descent phenomenon}
  \begin{columns}
    \begin{column}{0.5\textwidth}
      %\vspace*{0.2cm}
      Take again the algebraic equations:
      \vspace*{-0.2cm}
      $$
      \begin{cases}
        -\lambda g^2 + (1 - \lambda - \phi) g +1 = 0 \\
        -\lambda (2gg' + g^2) + (1 - \lambda - \phi) g' + g = 0
      \end{cases}
      $$

      %\vspace*{0.1cm}

      Then in the limit $\lambda \to 0$:
      \begin{align*}
        \mathcal B_X(\hat \beta) & = \begin{cases} r^2 (1-\phi)  & \text{if } \phi < 1\\
        0 & \text{if } \phi > 1
        \end{cases}\\
        \mathcal V_X(\hat \beta) & = \begin{cases} \sigma^2 \frac{\phi}{1-\phi} & \text{if } \phi < 1\\
        \sigma^2 \frac{1}{\phi - 1} & \text{if } \phi > 1
        \end{cases}
      \end{align*}
    \end{column}
    \begin{column}{0.5\textwidth}
      % image marchenko-pastur.png with caption "\phi = 2"
      \vspace*{0.2cm}
      \begin{figure}
        \centering
        \includegraphics[width=\textwidth]{images/bias-variance.white.png}
        \caption{Test error $\sigma^2 = 0.1, r=1$}
      \end{figure}
    \end{column}
  \end{columns}

  \begin{alertblock}{Conclusion \cite{belkin2020two,Hastie-Montanari-2019}}
  \begin{equation*}
    \lim_{\lambda \to 0}\Egen(r,\sigma,\phi,\lambda) = \begin{cases}
      \sigma^2 + r^2 (1-\phi) + \sigma^2 \frac{ \phi}{1 - \phi} & \text{if } \phi < 1   \\
      \sigma^2 + \sigma^2 \frac{1}{\phi - 1} & \text{if } \phi > 1
  \end{cases}
  \end{equation*}
  \end{alertblock}
\end{frame}




\begin{frame}{More general models?}

  \emph{Marchenko-Pastur works well in that case, but what about more complex models?}

  \textbf{Non-exhaustive list of interesting models:}

  \begin{enumerate}
    \item Teacher-student model $Y = X \beta^*$ and $\hat Y = \hat X \beta$ with covariation structure $\Sigma = \mean[ x \hat x^T]$
    \item Random feature model: $\hat Y = \sigma(WX) \beta$ to have a notion of number of parameters
    \item Spike Wigner model: $Y = \beta^{*} \beta^{*T} + \xi$ 
    \item Matrix denoising: $Y = XX^T + \xi$
  \end{enumerate}

  \vspace*{0.3cm}

  \begin{block}{Is there a practical tool to analyze them?}
    \begin{center}
      \textbf{Linear-Pencils!}
    \end{center}
  \end{block}
\end{frame}

\section{Linear-Pencil Method}

\begin{frame}{Level 1: Linear Pencil, the basics}

  Let $S$ be a symmetric random matrix ($S_{ij} \sim \mathcal N(0, \frac{1}{d})$) and define Stieltjes transform of the spectral density $\rho$ of $S$:
  $$ g(z) = \int \frac{\rho(\lambda) \dd \lambda}{\lambda - z} = \traceLim[d]{ (S - zI)^{-1}}$$

  \begin{block}{Main result \cite{wigner1958distribution}}
    \begin{equation}\label{eq:trivial-linear-pencil}
        g = (z-g)^{-1}
    \end{equation}
  \end{block}

  \textbf{Note 1:} above eqn. often represented under the form:
  $$g^2 - zg + 1 = 0$$

  \textbf{Note 2:} 
  $L=(S-zI)$ \emph{is} a "trivial" linear-pencil of size $1 \times 1$! 
  Equation \eqref{eq:trivial-linear-pencil} will follow us in the next slides.

\end{frame}


\begin{frame}{Linear-Pencil: 3 examples}
  Illustration of the method with 3 examples:
  \begin{enumerate}
    \item Marchenko-Pastur:
    $$ g(z) = \traceLim[d]{(X^TX-zI_d)^{-1}}$$
    \item Sample Covariance Matrix:
    $$ g(z) = \traceLim[d]{(C^\frac12 X^TX C^\frac12 -zI_d)^{-1}}$$
    \item Random Feature Kernel:
    $$ g(z) = \traceLim[d]{(\sigma(W X) \sigma(W X)^T-zI)^{-1}}$$
  \end{enumerate}

  \begin{block}{}
    Each case involves algebraic operations on a matrix level.
  \end{block}
\end{frame}


\begin{frame}{The linearization method}
  Back to Marchenko-Pastur: $L = (X^TX - zI)$ is not a linear-pencil, because it is not "linear" in $X$.
  
  \textbf{But} we can linearize it with a block-matrix!

  Consider $U_1 = (X^TX - zI)^{-1}$ with:
  \begin{align*}
    \begin{cases}
      (X^TX - zI) U_1 & = I\\
      U_2 & = X U_1\\
    \end{cases}
  \end{align*}
  \vspace*{-0.1cm}
  Then we obtain our linear-pencil $L$:

  \begin{equation*}
    \underbrace{
    \begin{pmatrix}
      -zI & X^T\\
      X & -I
    \end{pmatrix}
    }_{L}
    \begin{pmatrix}
      U_1\\
      U_2
    \end{pmatrix}
    = \begin{pmatrix}
      I\\
      0
    \end{pmatrix}
  \end{equation*}
  And in particular, $\traceLim[d]{U_1}$ is given by the partial trace of $L^{-1}$ because:
  \begin{equation*}
    U_1 = \begin{pmatrix}
      I & 0
    \end{pmatrix}
    (L^{-1})
    \begin{pmatrix}
      I\\
      0
    \end{pmatrix}
    = (L^{-1})^{(11)}
  \end{equation*}
\end{frame}

\begin{frame}{Linear Pencil: main result}
  \begin{columns}[T]
    \begin{column}{0.45\textwidth}
  \textbf{Step 1:} Encode matrix into a wider block-matrix (variance $\frac{1}{d}$):
  \begin{equation*}
    L = \begin{pmatrix}
      -z I_d & X^T\\
      X & - I_n
    \end{pmatrix}
  \end{equation*}
\end{column}
\begin{column}{0.45\textwidth}

  \textbf{Step 2:} Extract deterministic blocks from random matrices:
  \begin{equation*}
    L_0 = \begin{pmatrix}
      -z I_d& 0\\
      0 & -I_n
    \end{pmatrix}
  \end{equation*}
  \end{column}
  \end{columns}
    
  \textbf{Step 3:} Calculate block-wise covariance structure of $L$:
  $$
  \sigma_{12}^{21} = \sigma_{21}^{12} = 1 
  \qquad
  \qquad
  \sigma_{11}^{11} = \sigma_{22}^{22} = 0
  $$


  \textbf{Step 4:} Define the partial-trace of the inverse, and interesting operator:
  \begin{equation*}
    g_{ij} = \traceLim[d]{
      (L^{-1})^{(ij)}
    }
    \qquad \qquad
    [\eta_L(g)]_{il} = \sum_{jk \in \mathbb S} \sigma_{ij}^{kl} g_{jk}
  \end{equation*}

  Here, $g_{11}$ is clearly the quantity of interest, and:
  \begin{equation*}
    \eta_L(g) = \begin{pmatrix}
      \sigma_{12}^{21} g_{22} & 0\\
      0 & \sigma_{21}^{12} g_{11}
    \end{pmatrix}
  \end{equation*}
\end{frame}

\begin{frame}{Linear Pencil: Main result}
  \begin{block}{Main result \cite{spectra,mingo2017free}}
    \begin{equation}\label{eq:main-result-linear-pencil}
        g = \traceLimOp[d]{
            (L_0 - (\eta_L \otimes I)(g))^{-1}
        }
    \end{equation}
  \end{block}
\begin{enumerate}
  \item $(\eta_L \otimes I)(\cdot)$ is the matrix operator acting on $g$:
  \begin{equation*}
    ((\eta_L \otimes I)(g))^{(ij)} = 
    \begin{cases}
      [\eta_L(g)]_{ij} I_{N_i} & \text{if } N_i=N_j\\
      0_{N_i,N_j} & \text{otherwise}
    \end{cases}
  \end{equation*}
  \emph{So in our example:}
  \begin{equation*}
    L_0 - (\eta_L \otimes I)(g) =
    \begin{pmatrix}
      -zI_d - [\eta_L(g)]_{11} I_d & 0\\
      0 & -I_n -  [\eta_L(g)]_{22} I_n
    \end{pmatrix}
  \end{equation*}

  \item   $J \odot \traceLim[d]{\cdot}$ is the partial trace operator:
  \begin{equation*}
    \left[\traceLimOp[d]{G}\right]_{ij} = 
    \begin{cases}
      \traceLim[d]{G^{(ij)}} & \text{if } N_i=N_j\\
      0 & \text{otherwise}
    \end{cases}
  \end{equation*}
  \end{enumerate}
    
\end{frame}


\begin{frame}{Linear Pencil, example 1: Marchenko-Pastur}
  \begin{itemize}
    \item From \textbf{step 4} we have defined:
    \begin{equation}
      g_{11} = \traceLim[d]{ (X^TX - zI_d)^{-1}}
    \end{equation}
    \item From application of \textbf{Main result} we find:
    \begin{equation*}
      \begin{pmatrix}
        g_{11} & 0\\
        0 & g_{22}
      \end{pmatrix}
      = \traceLimOp[d]{
        \begin{pmatrix}
          -zI_d - g_{22} I_d & 0\\
          0 & -I_n - g_{11} I_n
        \end{pmatrix}^{-1}
      }
      \end{equation*}
  \end{itemize}

  \begin{alertblock}{Conclusion}
    With $g_C(z) = \traceLim[d]{ (C - zI_d)^{-1}}$:
    \begin{equation*}
      \begin{cases}
        g_{11} = \traceLim[d]{ (-zI_n - g_{22}I_n)^{-1} } = - (z + g_{22})^{-1}\\
        g_{22} = \traceLim[d]{ (-I_n - g_{11}I_n)^{-1} }
        = - \phi \left( 
          1 + g_{11} 
          \right)^{-1}
      \end{cases}
    \end{equation*}
  \end{alertblock}

  $$
    g_{11} = -\frac{1}{z - \phi (1+g_{11})^{-1}}
    \implies
    z g_{11}^2 + (1+z-\phi) g_{11} +1 = 0
  $$

\end{frame}


\begin{frame}{Linear-pencil, example 2: Sample covariance Matrix}
  \emph{Linear-pencil works for more general matrices: for instance, the sample covariance matrix.}

  Quite often, real-world problems come from samples $u \sim \mathcal N(0, C)$.
  An equivalent model is $u = C^{\frac12}x$ with $x \sim \mathcal N(0, I_d)$.

\begin{block}{Problem}
  Given $n$ samples $U=(u_1, \ldots, u_n)$ an estimation of $C$ is given by:
  $$ \tilde C = \frac{1}{d} UU^T = C^{\frac12} X X^T C^{\frac12}$$ with $X$ a random matrix.
  How to characterize the eigenvalue distribution of $\tilde C$ compared to $C$?
\end{block}

  Linear-pencils allow the study of the Stieltjes transform of $\tilde C$:
  $$
  g(z) =  \traceLim[d]{(C^{\frac12} X X^T C^{\frac12} - zI_d)^{-1} }
  $$
\end{frame}

\begin{frame}{Linear-pencil, example 2: Sample covariance Matrix}
  Let's consider instead:
  $$ g(z) = \traceLim[d]{(X^T C X - zI_n)^{-1}}$$
  In a similar way with $h(z) = \traceLim[d]{U_1}$ and the linearization:
  \begin{equation*}
    \underbrace{ \begin{pmatrix}
      -zI_n & 0 & X\\
      0 & C & I_d \\
      X^T & I_d & 0
    \end{pmatrix}}_{L}
    \begin{pmatrix}
      U_1 \\
      U_2 \\
      U_3
    \end{pmatrix}
    = \begin{pmatrix}
      I_n\\
      0\\
      0
    \end{pmatrix}
  \end{equation*}
  and with $\sigma_{13}^{31} = \sigma_{31}^{13} = 1$:
  \begin{equation*}
    L_0 = \begin{pmatrix}
      -zI_n & 0 & 0\\
      0 & C & I\\
      0 & I & 0
    \end{pmatrix}
    \qquad
    \text{and}
    \qquad
    \eta_L(g) = \begin{pmatrix}
      g_{33} & 0 & 0\\
      0 & 0 & 0\\
      0 & 0 & g_{11}
    \end{pmatrix}
  \end{equation*}
\end{frame}


\begin{frame}{Linear-pencil, example 2: Sample covariance Matrix}
  \begin{equation*}
    M= (L_0 - (\eta_L \otimes I)(g))^{-1} = 
    \begin{pmatrix}
      (-z - g_{33}) I_n & 0 & 0\\
      0 & C  & I_d\\
      0 & I_d & -g_{11} I_d
    \end{pmatrix}^{-1}
  \end{equation*}
  Using a block-matrix inversion formula:
  \begin{equation*}
    M = 
    \begin{pmatrix}
      -(z + g_{33})^{-1} I_n & 0 & 0\\
      0 & (C+g_{11}^{-1})^{-1} & (g_{11} C + I_d)^{-1}\\
      0 & (g_{11} C + I_d)^{-1} & -g_{11}^{-1} (I_d - (g_{11} C+ I_d)^{-1})
   \end{pmatrix}
  \end{equation*}
  Thus, formula $g = \traceLimOp[d]{M}$ from \textbf{Main result} yields:
  \begin{equation*}
    \begin{cases}
      g_{11} & = g(z) = - \phi (z+ g_{33})^{-1}\\
      g_{33} & = - \frac{1}{g_{11}} (1 - \traceLim[d]{ (g_{11} C + I_d)^{-1}})
      = \frac{1}{g(z)} ( \frac{1}{g(z)} 
      g_C(-\frac{1}{g(z)}) - 1)
    \end{cases}
  \end{equation*}
  \begin{block}{}
      $$
        \phi + z \phi g(z) + \frac{1}{g(z)} 
        g_C \left(\frac{-1}{g(z)}\right) = 1 
      $$
  \end{block}
\end{frame}


%\begin{frame}{Linear-pencil, example 3: Gaussian Equivalence Principle}
%  Consider a 1-Layer neural network with frozen random weight matrix $W$:
%  \begin{equation*}
%    \hat Y = \sigma(WX) \hat \beta
%  \end{equation*}
%  The ridge-regression estimator gives again:
%  $$ \hat \beta = \sigma(WX)^T (\sigma(WX) \sigma(WX)^T + \lambda I) Y$$
%  As for the toy model of the beginning: we are left to calculate the high-dimensional
%  eigenvalue distribution of $M=\sigma(WX) \sigma(WX)^T$, or similarly its Stieltjes transform:
%  $$g(z) = \traceLim[d]{(\sigma(WX) \sigma(WX)^T - zI_d)^{-1}}$$
%\end{frame}


\begin{frame}{Linear-pencil, example 3: Gaussian Equivalence Principle}
  How to calculate $g(z) = \traceLim[d]{(\sigma(WX) \sigma(WX)^T - zI_d)^{-1}}$?
  \begin{block}{GEP \cite{pennington2017nonlinear}}
    \begin{equation}
      \sigma(WX) \equiv
      \mu W X + \nu \Omega
    \end{equation}
    With:
    $
      0 = \langle \sigma, H_{e_0} \rangle
      \qquad
      \qquad
      \mu = \langle \sigma, H_{e_1} \rangle
      \qquad
      \qquad
      \mu^2 + \nu^2 = \langle \sigma, \sigma \rangle
    $
  \end{block}
  Ex.: $U = (\mu W, \nu \Omega), V = (X^T, I)$ so $g(z) = \traceLim[d]{(U V^T V U^T - zI_d)^{-1}}$
  \begin{equation*}
    L = \begin{pmatrix}
      -z I & 0 & 0 & U\\
      0 & 0 & V^T & I\\
      0 & V & I & 0\\
      U^T & I & 0 & 0
    \end{pmatrix}
    = \begin{pmatrix}
      -z I & 0 & 0 & 0 & \mu W & \nu \Omega\\
      0  & 0 & 0 & X & I & 0\\
      0  & 0 & 0& I & 0 & I\\
      0 & X^T & I & I & 0 & 0\\
      \mu W^T  & I & 0 & 0 & 0 & 0\\
      \nu \Omega^T & 0 & I & 0 & 0 & 0
    \end{pmatrix}
  \end{equation*}
  \textbf{Note:} no unique encoding! In fact, possible with a $4\times 4$ linear-pencil
\end{frame}



\begin{frame}{GEP: Construction of $4\times 4$ linear pencil}
  Consider $\traceLim[d]{(\sigma(XW) \sigma(XW)^{T} - zI)^{-1}} = \traceLim[d]{U_0}$:
  \begin{align*}
    ((\mu W X + \nu \Omega) (\mu X^T W^T + \nu \Omega^T) - z I) U_0 & = I\\
    U_1 & = \mu W^T U_0 \\
    U_2 & = X^T U_1 + \nu \Omega^T U_0\\
    U_3 & = X U_2
  \end{align*}
  So first equation is: $ \mu W U_3 + \nu \Omega U_2 - z U_0 = I$, so:
  \begin{align*}
    \begin{pmatrix}
      -z I & 0 & \nu \Omega & \mu W\\
      0 & 0 & X & -I\\
      \nu \Omega^T & X^T & -I & 0\\
      \mu W^T & -I & 0 & 0
    \end{pmatrix}
    \begin{pmatrix}
      U_0\\
      U_1\\
      U_2\\
      U_3
    \end{pmatrix}
    = 
    \begin{pmatrix}
      I\\
      0\\
      0\\
      0
    \end{pmatrix}
  \end{align*}

\end{frame}

\section{The dynamics of general linear models}

\begin{frame}{Random Feature Model is just a structured linear model}
  \begin{block}{Gaussian covariate model \cite{loureiro2021capturing}}
    A mutual source of randomness $Z$, and two structures $A$ and $B$:
    \begin{equation*}
      Y = X \beta^* = (ZB) \beta^* \qquad \qquad \hat Y = \tilde X \beta = (Z A) \beta
    \end{equation*}
    With the generalization error:
    \begin{equation*}
      \Egen(\beta) = \mean \left[ ( x^T \beta^* - \tilde x^T \beta)^2 \right]
    \end{equation*}
  \end{block}

  \textbf{Ex.} Random feature model with $Z= (X_0 \quad \Omega \quad \xi)$ and:
  \begin{equation*}
    \beta^* = \begin{pmatrix}
      \beta_0^*\\
      1
    \end{pmatrix}
    \qquad
    B = \begin{pmatrix}
      I_d & O\\
      O & O\\
      0 & \sigma
    \end{pmatrix}
    \qquad
    A=\begin{pmatrix}
      \mu W \\
      \nu I_{N}\\
      0
    \end{pmatrix}
  \end{equation*}
  Then:
  \begin{equation*}
    Y = X_0 \beta_0^* + \xi \qquad \qquad \hat Y = \left( \mu W X_0 + \nu \Omega \right) \beta
    \equiv \sigma(WX_0) \beta
  \end{equation*}
\end{frame}

\begin{frame}{The dynamics of the Gaussian covariate model}
  \begin{block}{Main result \cite{bodin.2212.06757}}
    
  \resizebox{\textwidth}{!}{
    \begin{minipage}{1.3\textwidth}
      With gradient flow on: $\mathcal E_{\text{train}}^\lambda(\beta) = \frac{1}{n} \simplenorm{Y - \hat Y(\beta)}^2 + \frac{\lambda}{n} \norm{\beta}^2$
      \begin{equation*}
        \bar{\mathcal E}_{\text{gen}}(t) = c_0 + r_0^2 \mathcal B_0(t) + \mathcal B_1(t)
      \end{equation*}   
      With random initialization $\beta(t=0) \sim \mathcal N(0, r_0^2\frac{1}{d} I_{d})$ :
      \begin{align*}
        \mathcal B_1(t) & =
        \frac{-1}{4 \pi^2}
        \oint_\Gamma \oint_\Gamma
        \frac{
            (1-e^{-t(x+\lambda)})
            (1-e^{-t(y+\lambda)})
        }{
            (x + \lambda)(y + \lambda)
        } f_1(x,y) \dd x \dd y
         +
        \frac{1}{i \pi}
        \oint_\Gamma
        \frac{1-e^{-t(z+\lambda)}}{z + \lambda}
        f_2(z)
        \dd z\\
        \mathcal B_0(t) & =
        \frac{-1}{2i\pi}
        \oint_\Gamma
        e^{-2 t(z+\lambda)} f_0(z)
        \dd z
      \end{align*}
      And with $V=B \beta^{*} \beta^{*T} B^T$, $U = A A^T$, $c_0 = \traceLim[d]{V}$ and the self-consistent equations:
      \begin{align*}
        f_1(x,y) & = f_2(x) + f_2(y) + \tilde f_1(x,y) - c_0\\
        \tilde f_{1}(x,y) & = \traceLim[d]{
            (\phi U + \zeta_x I)^{-1} (\zeta_x \zeta_y V^* + \tilde f_{1}(x,y) \phi U^2 ) (\phi U + \zeta_y I)^{-1}
        }\\  
        f_2(z) & = c_0 - \traceLim[d]{\zeta_z V^* (\phi U + \zeta_z I)^{-1}}\\
        f_0(z) & = -\left(1+\frac{\zeta_z}{z}\right)\\
        \zeta_z & = -z + \traceLim[d]{ \zeta_z U (\phi U + \zeta_z I)^{-1}}
      \end{align*}    
    \end{minipage}
  }
\end{block}
\end{frame}

\begin{frame}{The limit $t\to+\infty$ of the Gaussian covariate model}
  \begin{block}{Secondary result \cite{bodin.2212.06757}}
    \begin{align*}
      \bar{\mathcal E}_{\text{gen}}(+ \infty) & = c_0 + f_1(-\lambda, -\lambda) + 2f_2(-\lambda) = \tilde f_1\\
      \bar{\mathcal E}_{\text{train}}^0 (+\infty) & = \lambda^2 \zeta^{-2} \bar{\mathcal E}_{\text{gen}}(+\infty)
    \end{align*}
    With $U_\star = \phi A^TA$ and $\Xi = \phi A^T B$ and:
    \begin{align*}
      f_1 & = 2 f_2 + \tilde f_1 - c_0\\
      f_1 & =  \traceLim[n]{ ( (\Xi \beta^* \beta^{*T} \Xi^T) + \tilde f_{1} U_\star ) U_\star (U_\star + \zeta I)^{-2}}\\
      f_2 & = \traceLim[n]{ (\Xi \beta^* \beta^{*T} \Xi^T) ( U_\star + \zeta I)^{-1}} \\
      \zeta & = \lambda + \traceLim[n]{ \zeta U_\star ( U_\star + \zeta I)^{-1}}
    \end{align*}
  \end{block}
  See also \cite{loureiro2021capturing}
\end{frame}

\begin{frame}{Example 1: Random feature - model/sample double-descents}

  \textbf{Ex.:} \href{http://127.0.0.1:8000/output.mp4}{Triple descent} \cite{dascoli2020triple, bodin2021model} 

  \vspace*{-0.2cm}
  \begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/config4_2D.white.png}
    \caption{Parameters: $(\mu,\nu,\psi,r,s,\lambda) = (0.9,0.1,2,1,0.8,0.0001)$}
  \end{figure}
  \vspace*{-0.8cm}
  \begin{figure}[h!]
    \centering
    \includegraphics[width=1.0\textwidth]{images/config2_2D.white.png}
    \caption{Parameters $(\mu,\nu,\phi,r,s,\lambda) = (0.5,0.3,3,2.,0.4,0.001)$.
    }
  \end{figure}
\end{frame}

\begin{frame}{Example 2: Non-Isotropic regression - Descent structures}
  \vspace*{-0.5cm}
  \begin{equation*}
  B=I \quad \text{and} \quad A = \text{diag}(\underset{\frac{d}{p} \text{times}}{\underbrace{\alpha^{-\frac{0}{2}}, \ldots, \alpha^{-\frac{0}{2}}}}, 
  \underset{\frac{d}{p} \text{times}}{\underbrace{\alpha^{-\frac{1}{2}}, \ldots, \alpha^{-\frac{1}{2}}}}, \alpha^{-\frac{2}{2}}, \ldots, \alpha^{-\frac{p-1}{2}})
  \end{equation*}

  \vspace*{-0.5cm}
  \begin{columns}
    \begin{column}{0.5\textwidth}

    \begin{center}

      \begin{tikzpicture}
        \node[draw=none,shade,
            top color=grey,
            bottom color=grey,
            rounded corners=3pt,
            blur shadow={shadow blur steps=5}
        ] {\includegraphics[width=0.5\textwidth]{images/paper-triple.png}};
      \end{tikzpicture}
      \cite{nakkiran2020optimal}
    \end{center}

    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{figure}
        \centering
            \includegraphics[width=\textwidth]{images/multiple-3descents.png}
        \caption{\footnotesize $p=4, \lambda=10^{-13}$, $\alpha = 10^4$
        }
      \end{figure}
    \end{column}
  \end{columns}


  \begin{block}{Result with $\alpha \to +\infty$ \cite{bodin.2212.06757}}
  \vspace*{-0.25cm}
    \begin{equation*}\footnotesize
      \bar{\mathcal E}_{\text{gen}}(+\infty) = \frac{1}{p} \sum_{k=0}^{p-1}
      \frac{\phi (1-\phi)}{
          \left( \phi - \frac{k}{p} \right) \left( \frac{k + 1}{p} - \phi \right) 
      }  \mathds{1}_{\left] \frac{k}{p}; \frac{k+1}{p} \right[}(\phi) - \phi + o_\alpha(1)
  \end{equation*}
  \vspace*{-0.25cm}
  \end{block}

\end{frame}


\begin{frame}{Example 3: Realistic datasets} 
  \begin{itemize}
    \item \textbf{Data:} MNIST dataset $X_0 \in \mathbb R^{n_{tot} \times d}$ with normalized entries and $n_{tot}=60'000$ and $d=28\times 28 = 784$
    \item \textbf{Aim:} Learning the parity ($y=\pm1$ for even/odd numbers)
    \item \textbf{Structure:} cov. matrix $U_\star \simeq \frac{1}{n_{tot}} X^TX$ and $\Xi \beta^* \simeq X^TY$ (since $X^TX = A^T (Z^T Z) A$ and $X^T Y = A (Z^T Z) B \beta^*$ and $n_{tot} \gg d$)
  \end{itemize}
  \vspace*{-0.7cm}
  \begin{figure}
    \centering
    \subfloat{
        \includegraphics[width=0.5\textwidth]{images/lsq_profile.png}
    }
    \subfloat{
        \includegraphics[width=0.5\textwidth]{images/evolution1.png}
    }
    \caption{\footnotesize 
        Comparison between the analytical and experimental learning profiles for the minimum least-squares estimator at $\lambda = 10^{-3}$ on the left (20 runs) and the time evolution at $\lambda = 10^{-2}, n=700$ on the right (10 runs).
    }
  \end{figure}
\end{frame}



\begin{frame}[standout]
  Questions?
\end{frame}



\begin{frame}{Appendix: Gaussian Equivalence Principle}
  From \cite{lu2022equivalence,mei2020generalization} with $y,x \in \mathbb S^{d-1}$ , decomposition with Gegenbauer polynomials:
  \begin{equation*}
    \sigma(\sqrt{d} x^T y) = \sum_{k=1}^{p} \alpha_k q_k(\sqrt{d} x^T y)
  \end{equation*}
  (In high-dimenion, $\alpha_k$ are related to the coefficients of the Hermite polynomials decomposition).\\
  Decoupling with spherical harmonics $Y_{k,a}$:
  \begin{equation*}
    q_k(\sqrt{d} x^T y) = \frac{1}{\sqrt{N_k}} \sum_{a=0}^{N_k} Y_{k,a}(x) Y_{k,a}(y)
  \end{equation*}
\end{frame}
  


\begin{frame}{Appendix: Bias-variance decomposition}
  Classical bias-variance decomposition over new data point $x$:
  \begin{align}
    \mean[(y - \hat y )^2] & = \mean[y - \hat y]^2 + \var(\hat y) + \var(y)
  \end{align}
  Is true when:
  \begin{align}
    \var(y - \hat y) = \var(\hat y) + \var(y)
    \implies \cov(y, \hat y)=0
  \end{align}
  This is true for the R.V. $\beta_0, \xi, X, \epsilon$ (because each of them only affect either $y$ and $\hat y$) but not for $x$ (and $\beta^*$ if considered random). So in fact we implicitly always require the conditional expectation on $x$:
  \begin{align}
    \mean[(y - \hat y )^2] & = \mean_x\left[ \mean[y - \hat y| x]\right]^2 + \mean_x \var(\hat y | x) + \mean_x \var(y | x)
  \end{align}
\end{frame}




\appendix

\begin{frame}[allowframebreaks, noframenumbering]{References}

  \bibliographystyle{apalike}
  \bibliography{bibliography}

\end{frame}

\end{document}
